{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20163a38-27a3-4f3d-8d71-8d3f7d16b630",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "https://www.sbert.net/examples/applications/semantic-search/README.html\n",
    "\n",
    "https://www.sbert.net/docs/pretrained_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3cc9ec-9d52-4561-9b33-1997f0fb9b83",
   "metadata": {},
   "source": [
    "In this notebook, I loaded datasets containing training, evaluation, and test samples for a question answering task. After extracting questions and contexts from the training and evaluation datasets, I utilized the SentenceTransformer library to load a pre-trained model for encoding text into embeddings. Using this model, I encoded the training and evaluation questions, normalized the embeddings, and employed semantic search to find the most similar question in the training set for each evaluation question. Subsequently, I calculated the accuracy of this model in identifying the correct context for evaluation questions and determined the best-performing model based on accuracy. Furthermore, I authenticated with the Hugging Face Hub and imported necessary libraries to work with a specific pre-trained model called Mistral AI `mistralai/Mistral-7B-v0.1`. After defining its configuration and loading its weights for causal language modeling, I loaded a previously trained and fine-tuned model from a checkpoint directory. To ensure reproducibility, I set a random seed and selected a random test sample from the test dataset. Then, I encoded the test query, found the most similar question in the training set, and retrieved its context. Using this information, I created a prompt and tokenized it for model input. Finally, I generated text based on the model input, disabled gradient calculation during inference, and decoded the generated text to produce a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e404e5-d8c1-493a-8e50-63140608d09b",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "178e6420-6817-4874-9dc0-19756df8808d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c0b0a5d9d148d5b12d6873851e1d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb50d7b6dff54273bfecaa4c7113b339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b116e7fad60f4fed8f1d9166a1711243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load training dataset\n",
    "train_dataset = load_dataset('json', data_files='../data/train_CRM_data.json', split='train')\n",
    "\n",
    "# Load evaluation dataset\n",
    "eval_dataset = load_dataset('json', data_files='../data/val_CRM_data.json', split='train')\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = load_dataset('json', data_files='../data/test_CRM_data.json', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ce6a8-3553-4411-afc1-6f0010bd3311",
   "metadata": {},
   "source": [
    "# Extract Questions and Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aef2c56-ed65-48cd-8851-281d7b051df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_question_context_dict(message):\n",
    "    \"\"\"\n",
    "    Create a dictionary with question as key and context as value\n",
    "\n",
    "    Args:\n",
    "        message (list): A list containing dictionaries for question, context, and answer.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of question as key and context as value\n",
    "    \"\"\"\n",
    "    # Extracting question, context, and answer from the message\n",
    "    question = message[1]['content']\n",
    "    context = message[0]['content']\n",
    "    answer = message[2]['content']\n",
    "\n",
    "    return {question: context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62195c60-05f2-430c-b864-8e3f513f6392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_question_context_dict = {}\n",
    "\n",
    "# Iterate through each example in the training dataset\n",
    "for sample in train_dataset:\n",
    "    \n",
    "    # Extract question and context using create_question_context_dict function\n",
    "    qc_pair = create_question_context_dict(sample['messages'])\n",
    "    \n",
    "    # Merge the extracted pairs into the train_question_context_dict\n",
    "    train_question_context_dict.update(qc_pair)\n",
    "\n",
    "eval_question_context_dict = {}\n",
    "\n",
    "# Iterate through each example in the evaluation dataset\n",
    "for sample in eval_dataset:\n",
    "    \n",
    "    # Extract question and context using create_question_context_dict function\n",
    "    qc_pair = create_question_context_dict(sample['messages'])\n",
    "    \n",
    "    # Merge the extracted pairs into the eval_question_context_dict\n",
    "    eval_question_context_dict.update(qc_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05693c07-0788-4f94-b0c2-8b1de6c5c9b4",
   "metadata": {},
   "source": [
    "# Load SentenceTransformer Model, Encode Training Questions, Encode Evaluation Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2de1778-6f44-4889-bff6-ee7c904fb0e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Load pre-trained SentenceTransformer model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Extracting all questions from train_question_context_dict\n",
    "train_questions_corpus = list(train_question_context_dict.keys())\n",
    "\n",
    "# Encode the questions into embeddings\n",
    "train_corpus_embeddings = embedder.encode(train_questions_corpus, convert_to_tensor=True)\n",
    "\n",
    "# Move embeddings to GPU if available\n",
    "train_corpus_embeddings = train_corpus_embeddings.to(\"cuda\")\n",
    "\n",
    "# Normalize embeddings\n",
    "train_corpus_embeddings = util.normalize_embeddings(train_corpus_embeddings)\n",
    "\n",
    "# Query sentences:\n",
    "eval_questions_queries = list(eval_question_context_dict.keys())\n",
    "\n",
    "# Encode evaluation questions into embeddings\n",
    "query_embeddings = embedder.encode(eval_questions_queries, convert_to_tensor=True)\n",
    "\n",
    "# Move query embeddings to GPU if available\n",
    "query_embeddings = query_embeddings.to(\"cuda\")\n",
    "\n",
    "# Normalize query embeddings\n",
    "query_embeddings = util.normalize_embeddings(query_embeddings)\n",
    "\n",
    "# Find the most similar question in the training set for each evaluation question\n",
    "hits = util.semantic_search(query_embeddings, train_corpus_embeddings, score_function=util.dot_score, top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e984e2-804c-4397-b47c-4c6989f6851b",
   "metadata": {},
   "source": [
    "# Find Similar Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc223aad-eea1-4f14-8fdf-1e0cde89e775",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Eval Question  \\\n",
      "0   What is the correlation between transaction sc...   \n",
      "1   Most Common Product Category Mentioned in Cust...   \n",
      "2   Are there any products with a spike in transac...   \n",
      "3   Could you evaluate the engagement predictions ...   \n",
      "4   Can you identify any outliers in transaction s...   \n",
      "5   Are there any products with consistently low t...   \n",
      "6          Can you identify the top-selling products?   \n",
      "7   How many transactions have occurred for each c...   \n",
      "8   Are there any products with a consistent incre...   \n",
      "9   Are there any trends or patterns in purchasing...   \n",
      "10  Are there any specific customer segments that ...   \n",
      "11  How many transactions in our database have a s...   \n",
      "12  Are there any products that are frequently pur...   \n",
      "13  Are there any products with a spike in transac...   \n",
      "14  Are there any trends in the rankings of produc...   \n",
      "15  Are there any outliers in terms of high-volume...   \n",
      "16  Are there any external factors or market trend...   \n",
      "17  Can you identify any patterns in transaction v...   \n",
      "18  What is the average transaction score for tran...   \n",
      "19  Can you find the most common product category ...   \n",
      "20  Can you provide me with the next best offer re...   \n",
      "21  How does customer satisfaction and loyalty imp...   \n",
      "22  Are there any trends or patterns in purchasing...   \n",
      "23  I'm interested in forecasting the purchases of...   \n",
      "24  Are there any seasonal trends or events that i...   \n",
      "25  How many transactions have occurred for each c...   \n",
      "26  How frequently are customers from different ci...   \n",
      "27  How many unique customers have made transactio...   \n",
      "28  Can you provide a breakdown of transactions by...   \n",
      "29  Can you provide insights into the effectivenes...   \n",
      "30  Can you provide rankings of products across di...   \n",
      "31  What is the distribution of product categories...   \n",
      "32  What is the average transaction score for tran...   \n",
      "33  What is our current churn rate and cost per ac...   \n",
      "34  Can you identify any outliers or anomalies in ...   \n",
      "35  Can you identify any outliers or anomalies in ...   \n",
      "36  Are there any outliers in terms of high-volume...   \n",
      "37     What is the average time between transactions?   \n",
      "38  What is the average score for transactions in ...   \n",
      "39  Can you provide a summary of transactions by c...   \n",
      "40  How many transactions have occurred for each d...   \n",
      "\n",
      "                         Best Matching Train Question  \\\n",
      "0   What is the correlation between transaction sc...   \n",
      "1                          Popular Product Categories   \n",
      "2   Are there any products with a spike in transac...   \n",
      "3   What is the comparison between the predicted e...   \n",
      "4   Can you identify any outliers in transaction v...   \n",
      "5   Are there any products with consistently high ...   \n",
      "6   What are the top-selling products in each prod...   \n",
      "7   How many transactions have occurred for each c...   \n",
      "8   Are there any products with a consistent incre...   \n",
      "9   Are there any seasonal or temporal trends that...   \n",
      "10  Can you provide insights into the effectivenes...   \n",
      "11         What is the average score of transactions?   \n",
      "12  Are there any specific products that tend to b...   \n",
      "13  Are there any products with a spike in transac...   \n",
      "14  Are there any trends in the ratings of product...   \n",
      "15  Can you identify outliers among our customers ...   \n",
      "16  Are there any trends or patterns in customer b...   \n",
      "17  Can you identify any patterns in transaction v...   \n",
      "18  What is the average transaction score for tran...   \n",
      "19  What is the most common product category purch...   \n",
      "20  Can you provide the next best offer recommenda...   \n",
      "21  How does customer engagement and interaction i...   \n",
      "22  Are there any external factors or market trend...   \n",
      "23  Can you provide a prediction of the top-rated ...   \n",
      "24  Are there any seasonal trends or events that i...   \n",
      "25  How many transactions have occurred for each c...   \n",
      "26  How does the frequency of purchases vary betwe...   \n",
      "27  How many transactions have occurred for each p...   \n",
      "28  Can you provide a summary of transactions by p...   \n",
      "29  Can you identify any trends or correlations be...   \n",
      "30  Can you provide the average score of transacti...   \n",
      "31  How many transactions have occurred for each p...   \n",
      "32  What is the average transaction score for tran...   \n",
      "33  Could you analyze the churn predictions for ou...   \n",
      "34  Are there any trends or patterns in search beh...   \n",
      "35  How do the engagement likelihood predictions a...   \n",
      "36  Can you identify outliers among our customers ...   \n",
      "37  What is the average time between transactions ...   \n",
      "38  What is the average transaction value for each...   \n",
      "39  Can you provide a summary of transactions by c...   \n",
      "40  How many transactions have occurred for each p...   \n",
      "\n",
      "    Correct Context Identified  \n",
      "0                         True  \n",
      "1                        False  \n",
      "2                         True  \n",
      "3                         True  \n",
      "4                         True  \n",
      "5                        False  \n",
      "6                         True  \n",
      "7                         True  \n",
      "8                         True  \n",
      "9                         True  \n",
      "10                        True  \n",
      "11                       False  \n",
      "12                       False  \n",
      "13                       False  \n",
      "14                       False  \n",
      "15                        True  \n",
      "16                        True  \n",
      "17                        True  \n",
      "18                       False  \n",
      "19                        True  \n",
      "20                        True  \n",
      "21                        True  \n",
      "22                        True  \n",
      "23                       False  \n",
      "24                       False  \n",
      "25                        True  \n",
      "26                        True  \n",
      "27                        True  \n",
      "28                        True  \n",
      "29                        True  \n",
      "30                        True  \n",
      "31                        True  \n",
      "32                        True  \n",
      "33                        True  \n",
      "34                        True  \n",
      "35                       False  \n",
      "36                       False  \n",
      "37                       False  \n",
      "38                        True  \n",
      "39                       False  \n",
      "40                       False  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_data = []\n",
    "\n",
    "# Iterate through each evaluation question and its corresponding hit\n",
    "for idx, eval_questions_query in enumerate(eval_questions_queries):\n",
    "    eval_question = eval_questions_query\n",
    "    \n",
    "    # Get the best matching train question using the hits\n",
    "    best_matching_train_question = train_questions_corpus[hits[idx][0]['corpus_id']]\n",
    "    \n",
    "    # Check if the correct context is identified by comparing the contexts of the best matching train question and evaluation question\n",
    "    correct_context_identified = train_question_context_dict[best_matching_train_question] == eval_question_context_dict[eval_questions_query]\n",
    "    \n",
    "    # Append the data to the output list\n",
    "    output_data.append({\n",
    "        'Eval Question': eval_question,\n",
    "        'Best Matching Train Question': best_matching_train_question,\n",
    "        'Correct Context Identified': correct_context_identified\n",
    "    })\n",
    "\n",
    "# Creating a DataFrame from the output data\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Displaying the DataFrame\n",
    "print(output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9ee238-c17e-435c-a309-696d40fcdb5c",
   "metadata": {},
   "source": [
    "# Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd7218d-156f-486e-94e7-35fa8835937d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6585365853658537\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of evaluations\n",
    "total_evaluations = len(output_df)\n",
    "\n",
    "# Calculate the number of correct context identifications\n",
    "correct_identifications = output_df['Correct Context Identified'].sum()\n",
    "\n",
    "# Calculate accuracy by dividing the number of correct identifications by the total evaluations\n",
    "accuracy = correct_identifications / total_evaluations\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced16ce-ad00-4e3a-8d25-b99a4398bce4",
   "metadata": {},
   "source": [
    "# Identify Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd20db4f-b0e2-46dc-9c75-d9c641e18477",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6278f1e0f5470cb83fa22f7a1d7c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e151de6eedde47379d87b013dcd78a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21d767514b34b9888da08d9e7999b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b510d646ae4dec8d36da3d6575b27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8121ce1279941b2b7fc84f506c858c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a20d33d8434bf3bd3d89780d79005d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1504da9eb44fdaab2abcbff185b5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e3f3577bed49cd8625daac5c230ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc26d7ed7fd14e948f45c4301f0bc397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3801c0d298e3471794c618c626827342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a094771f5945eebe3b5d33f6a68fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e649e6ebb40846d0ad8d65acb77aef4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92c4b2c732246a2a5c071a226ad475b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2793d8c2ba94757914ec2afafd51022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f927e64c264c65a8e20881178ebfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272fc04399204d9a95feb7aa0acd60a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15058e9fd3a4e1f97f08c0d2a2bd9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/9.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acf8b47dca04319b559bb6df6635962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5565e7e28d4f34ae3ed8ad86f9511d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bb9957b8814e49a9ceba4b84aa2417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9b7bb3ef8e411382bd205c2807c57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01659e7455b484f85aba76824c61611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2a99b2fa134853b5bbf7d3d13cf6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6be741420a84eec9ed1ce214ad16909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/model.safetensors:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ff72790f2d47e2be55a74521f4ec20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fbfa4b3807496eaea6b017c4d50672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a63216c9bd84266a83543c73518044a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca78d68502b845a78937d6e524167fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e58dba814441f58bf78d3b6d5ccb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012184cc570440b180c3e26541e11c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15708c6ebf641e79551a36927df4ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7802fbd3544326baf834766badebbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c86beb61a954901a57a845f9e3080b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a400cac6df644308bef42e3596f02f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd43a77f28a8409796fdd6197716acf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91580749a8f48ad85d8c4ac91b1d5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ec6fb3c65b4cbe96ed5d64e3f4061e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d821e236afa64b51af43d40248744f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/model.safetensors:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a9036333cb4db585a8db9dff5d593a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52da2a51af9c42bbaeb4f8a5df22a21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b056e99a3a46e0a3d841f990179fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1451105aa444d2fa70905b13acf92f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c64fdea37ad43b586e4673b4daf18d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7e8f7c44284a23ae2c72de42b9465c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f216a2ae1e8f4f629e773d14dd567bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/9.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e334542c7049e0a6815647760acb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4ba32d4ae84dd1a1a7d88fbd250993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f55f72be294cd989869a55e1c19baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d259e6a7700c4f28aa113cefbf395fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394eef6ee181492cb04154d7904e84a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4758120786474f94b5ccfc4d3f72e9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e1d5d23ffd40178041fb60dc661763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1252dd1ff5b14d6da5d110caede34261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/model.safetensors:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb2975a96ac4d6ea534504b2a1baeb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf189f2d5574432b839ef1a72a111fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d622b2749a46369830ebd038fd1060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052a8b1611414d68b448742ccd65a888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096b8b1553f64ea38e54c152c398f334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1006: UserWarning: Not enough free disk space to download the file. The expected file size is: 669.90 MB. The target location /home/jovyan/.cache/huggingface/hub only has 496.44 MB free disk space.\n",
      "  warnings.warn(\n",
      "/home/jovyan/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1006: UserWarning: Not enough free disk space to download the file. The expected file size is: 669.90 MB. The target location /home/jovyan/.cache/huggingface/hub/models--sentence-transformers--gtr-t5-large/blobs only has 496.44 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a1e9c2c932491d8ce423fdbcced655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Evaluate each model and store accuracy results\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m model_names:\n\u001b[0;32m---> 95\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_question_context_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_question_context_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     accuracy_results[model_name] \u001b[38;5;241m=\u001b[39m accuracy\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Print accuracy results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mevaluate_model_accuracy\u001b[0;34m(model_name, train_question_context_dict, eval_question_context_dict)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model_accuracy\u001b[39m(model_name, train_question_context_dict, eval_question_context_dict):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     embedder \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Extracting all questions from train_question_context_dict\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     train_questions_corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(train_question_context_dict\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:197\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, cache_folder, trust_remote_code, revision, token, use_auth_token, truncate_dim)\u001b[0m\n\u001b[1;32m    194\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(model_name_or_path, token, cache_folder\u001b[38;5;241m=\u001b[39mcache_folder, revision\u001b[38;5;241m=\u001b[39mrevision):\n\u001b[0;32m--> 197\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    206\u001b[0m         model_name_or_path,\n\u001b[1;32m    207\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m    211\u001b[0m     )\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1296\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1295\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_args\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hub_kwargs\n\u001b[0;32m-> 1296\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;66;03m# Normalize does not require any files to be loaded\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_class \u001b[38;5;241m==\u001b[39m Normalize:\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:36\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case \u001b[38;5;241m=\u001b[39m do_lower_case\n\u001b[1;32m     35\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     39\u001b[0m     tokenizer_name_or_path \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model_name_or_path,\n\u001b[1;32m     40\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_args,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# No max_seq_length set. Try to infer from model\u001b[39;00m\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:61\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, **model_args)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads the transformer model\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, T5Config):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_t5_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, MT5Config):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:74\u001b[0m, in \u001b[0;36mTransformer._load_t5_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, **model_args)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5EncoderModel\n\u001b[1;32m     73\u001b[0m T5EncoderModel\u001b[38;5;241m.\u001b[39m_keys_to_ignore_on_load_unexpected \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder.*\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mT5EncoderModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3251\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3237\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   3238\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   3240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   3250\u001b[0m     }\n\u001b[0;32m-> 3251\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3253\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   3254\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   3255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   3256\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:389\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1492\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1490\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[0;32m-> 1492\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1503\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:550\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n\u001b[0;32m--> 550\u001b[0m     \u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     new_resume_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;66;03m# Some data has been downloaded from the server so we reset the number of retries.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/tempfile.py:483\u001b[0m, in \u001b[0;36m_TemporaryFileWrapper.__getattr__.<locals>.func_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;129m@_functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_model_accuracy(model_name, train_question_context_dict, eval_question_context_dict):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of a sentence transformer model on a given set of training and evaluation question-context pairs.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the sentence transformer model to be used for embedding.\n",
    "        train_question_context_dict (dict): Dictionary mapping training questions to their corresponding contexts.\n",
    "        eval_question_context_dict (dict): Dictionary mapping evaluation questions to their corresponding contexts.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model in identifying the correct context for evaluation questions.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "\n",
    "    # Extracting all questions from train_question_context_dict\n",
    "    train_questions_corpus = list(train_question_context_dict.keys())\n",
    "    \n",
    "    # Encode the training questions into embeddings and move them to GPU if available\n",
    "    train_corpus_embeddings = embedder.encode(train_questions_corpus, convert_to_tensor=True).to(\"cuda\")\n",
    "    \n",
    "    # Normalize the embeddings\n",
    "    train_corpus_embeddings = util.normalize_embeddings(train_corpus_embeddings)\n",
    "\n",
    "    # Query sentences:\n",
    "    eval_questions_queries = list(eval_question_context_dict.keys())\n",
    "    \n",
    "    # Encode the evaluation questions into embeddings and move them to GPU if available\n",
    "    query_embeddings = embedder.encode(eval_questions_queries, convert_to_tensor=True).to(\"cuda\")\n",
    "    \n",
    "    # Normalize the embeddings\n",
    "    query_embeddings = util.normalize_embeddings(query_embeddings)\n",
    "\n",
    "    # Find the most similar question in the training set for each evaluation question\n",
    "    hits = util.semantic_search(query_embeddings, train_corpus_embeddings, score_function=util.dot_score, top_k=1)\n",
    "\n",
    "    output_data = []\n",
    "    for idx, eval_questions_query in enumerate(eval_questions_queries):\n",
    "        eval_question = eval_questions_query\n",
    "        \n",
    "        # Get the best matching train question using the hits\n",
    "        best_matching_train_question = train_questions_corpus[hits[idx][0]['corpus_id']]\n",
    "        \n",
    "        # Check if the correct context is identified by comparing the contexts of the best matching train question and evaluation question\n",
    "        correct_context_identified = train_question_context_dict[best_matching_train_question] == eval_question_context_dict[eval_questions_query]\n",
    "\n",
    "        output_data.append({\n",
    "            'Eval Question': eval_question,\n",
    "            'Best Matching Train Question': best_matching_train_question,\n",
    "            'Correct Context Identified': correct_context_identified\n",
    "        })\n",
    "\n",
    "    # Creating a DataFrame from the output data\n",
    "    output_df = pd.DataFrame(output_data)\n",
    "\n",
    "    # Calculate the total number of evaluations\n",
    "    total_evaluations = len(output_df)\n",
    "\n",
    "    # Calculate the number of correct context identifications\n",
    "    correct_identifications = output_df['Correct Context Identified'].sum()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_identifications / total_evaluations\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# List of models to evaluate\n",
    "model_names = [\n",
    "    \"all-mpnet-base-v2\",\n",
    "    \"gtr-t5-xxl\", \n",
    "    \"gtr-t5-xl\", \n",
    "    \"sentence-t5-xxl\",\n",
    "    \"gtr-t5-large\",\n",
    "    \"all-mpnet-base-v1\",\n",
    "    \"multi-qa-mpnet-base-dot-v1\",\n",
    "    \"multi-qa-mpnet-base-cos-v1\",\n",
    "    \"all-roberta-large-v1\",\n",
    "    \"sentence-t5-xl\",\n",
    "    \"all-distilroberta-v1\",\n",
    "    \"all-MiniLM-L12-v1\",\n",
    "    \"all-MiniLM-L12-v2\",\n",
    "    \"multi-qa-distilbert-dot-v1\",\n",
    "    \"multi-qa-distilbert-cos-v1\",\n",
    "    \"gtr-t5-base\",\n",
    "    \"sentence-t5-large\",\n",
    "    \"all-MiniLM-L6-v2\",\n",
    "    \"multi-qa-MiniLM-L6-cos-v1\",\n",
    "    \"all-MiniLM-L6-v1\",\n",
    "    \"paraphrase-mpnet-base-v2\",\n",
    "    \"msmarco-bert-base-dot-v5\",\n",
    "    \"multi-qa-MiniLM-L6-dot-v1\",\n",
    "    \"sentence-t5-base\",\n",
    "    \"msmarco-distilbert-base-tas-b\",\n",
    "    \"msmarco-distilbert-dot-v5\",\n",
    "    \"paraphrase-distilroberta-base-v2\",\n",
    "    \"paraphrase-MiniLM-L12-v2\",\n",
    "    \"paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"paraphrase-TinyBERT-L6-v2\",\n",
    "    \"paraphrase-MiniLM-L6-v2\",\n",
    "    \"paraphrase-albert-small-v2\",\n",
    "    \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"paraphrase-MiniLM-L3-v2\",\n",
    "    \"distiluse-base-multilingual-cased-v1\",\n",
    "    \"distiluse-base-multilingual-cased-v2\",\n",
    "    \"average_word_embeddings_komninos\",\n",
    "    \"average_word_embeddings_glove.6B.300d\"\n",
    "]\n",
    "\n",
    "# Dictionary to store accuracy results\n",
    "accuracy_results = {}\n",
    "\n",
    "# Evaluate each model and store accuracy results\n",
    "for model_name in model_names:\n",
    "    accuracy = evaluate_model_accuracy(model_name, train_question_context_dict, eval_question_context_dict)\n",
    "    accuracy_results[model_name] = accuracy\n",
    "\n",
    "# Print accuracy results\n",
    "for model_name, accuracy in accuracy_results.items():\n",
    "    print(f\"Model: {model_name}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe5a3da7-8d4c-480b-82bf-4a5eaa11c23b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model based on accuracy is 'all-mpnet-base-v2' with an accuracy of 73.17%.\n",
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# Find the model with the highest accuracy\n",
    "best_model = max(accuracy_results, key=accuracy_results.get)\n",
    "\n",
    "# Get the accuracy of the best model\n",
    "best_accuracy = accuracy_results[best_model]\n",
    "\n",
    "# Print the best model and its accuracy\n",
    "print(f\"The best model based on accuracy is '{best_model}' with an accuracy of {best_accuracy:.2%}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620ed08-e5de-4e47-a945-63f3e90b2c71",
   "metadata": {},
   "source": [
    "# Authenticate Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432a3393-0394-49ff-acf4-b2b9a4058ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93d27ccd06e406ebee6b08ab911ae43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the notebook_login function from huggingface_hub module\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Call the notebook_login function to authenticate\n",
    "# Provide an access token from the provided URL - https://huggingface.co/settings/tokens\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39cb892-4932-437d-b4f4-46564968f743",
   "metadata": {},
   "source": [
    "# Import Libraries and Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70e0104a-cbfa-42a5-9b33-4d7b2b9bf1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b102afc8249e47feafe39cf9c3c46f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4708da36394629b47895739557f59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e661ce0c1b24cb78fa7429287000b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e6c3201e33451fa5d469ca58d11b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fcd412cdcdf475fa6e70b235a8bc3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6b297048ec436a89583de572aa47f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f38dccc965f435dba41f75fd5c75d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf51220772ce404ab3395c1d52904c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32282f286cc4d05927971a488213c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7ec6f20edd4702a22f57b6d1ccb4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcecb035ffd4321ba7ab4e3c2b230c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfa92e46a64491d93416f356f5baef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# Define Mistral's pretrained model ID\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# Define BitsAndBytesConfig for Mistral's model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Load the model in 4-bit precision\n",
    "    bnb_4bit_use_double_quant=True,       # Use double quantization for 4-bit quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Use nf4 quantization for 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 for computation with 4-bit quantization\n",
    ")\n",
    "\n",
    "# Load Mistral's pretrained model for causal language modeling\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id, \n",
    "    quantization_config=bnb_config, # Apply the defined quantization configuration\n",
    "    device_map=\"auto\"               # Automatically select the device for model inference\n",
    ")\n",
    "\n",
    "# Load Mistral's tokenizer\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id, \n",
    "    add_bos_token=True,     # Add beginning-of-sequence token\n",
    "    trust_remote_code=True  # Trust remote code for tokenization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ebef3-bc8a-41dc-a9a8-5c0b2e101a32",
   "metadata": {},
   "source": [
    "# Load Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42053b98-7f93-42d9-adce-959728cbf4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load the weights from the checkpoint directory\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"../models/01-finetune-mistral/checkpoint-150\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8887ea6-89ef-4c8b-9b39-e0f980c7ad52",
   "metadata": {},
   "source": [
    "# Retrieve context for a random test sample question and prepare prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98338e12-ee76-43a2-a329-87d6aef023df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): lora.Linear(\n",
       "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=64, out_features=32000, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Take a random index\n",
    "random_index = random.randint(0, len(test_dataset) - 1)\n",
    "\n",
    "# Take the random sample\n",
    "random_test_sample = test_dataset[random_index]\n",
    "\n",
    "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Extracting all questions from train_question_context_dict\n",
    "train_questions_corpus = list(train_question_context_dict.keys())\n",
    "\n",
    "# Encode the training questions into embeddings and move them to GPU if available\n",
    "train_corpus_embeddings = embedder.encode(train_questions_corpus, convert_to_tensor=True).to(\"cuda\")\n",
    "\n",
    "# Normalize the embeddings\n",
    "train_corpus_embeddings = util.normalize_embeddings(train_corpus_embeddings)\n",
    "\n",
    "# Query sentence:\n",
    "test_query = [random_test_sample['messages'][1]['content']]\n",
    "\n",
    "# Encode the test query into embeddings and move them to GPU if available\n",
    "query_embedding = embedder.encode(test_query, convert_to_tensor=True).to(\"cuda\")\n",
    "\n",
    "# Normalize the embeddings\n",
    "query_embedding = util.normalize_embeddings(query_embedding)\n",
    "\n",
    "# Find the most similar question in the training set for the test query\n",
    "hits = util.semantic_search(query_embedding, train_corpus_embeddings, score_function=util.dot_score, top_k=1)\n",
    "\n",
    "# Retrieve the context corresponding to the most similar question\n",
    "retrieved_context = train_question_context_dict[train_questions_corpus[hits[0][0]['corpus_id']]]\n",
    "\n",
    "# Create a prompt with the test query and the retrieved context\n",
    "prompt = f\"\"\"\n",
    "### Question: {test_query[0]}\n",
    "\n",
    "### Context: {retrieved_context}\n",
    "### Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "# Init a tokenizer that doesn't add padding or eos token\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "# Tokenize the test prompt and prepare model input for inference\n",
    "model_input = test_tokenizer(\n",
    "    prompt, \n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")  # Move tensors to GPU if available\n",
    "\n",
    "# Set the fine-tuned model to evaluation mode\n",
    "ft_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1a467-72b9-4b2c-8a6d-bb43e61981ba",
   "metadata": {},
   "source": [
    "# Model Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4922a444-0c62-4f7c-bc35-f5008ab1bc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: What is the average transaction score for each product category?\n",
      "\n",
      "### Context: \n",
      "You are a Python function generator. Users will ask you questions in English, \n",
      "and you will produce a Python function as answer to the question based on the provided CONTEXT.\n",
      "\n",
      "CONTEXT:\n",
      "Pandas DataFrame df containing transaction data with columns order_id, user_id, item_id, timestamp, score.\n",
      "order_id takes string datatype and identifies the order.\n",
      "user_id takes string datatype and identifies the customer.\n",
      "item_id takes string datatype that identifies the product.\n",
      "timestamp takes timestamp datatype and represents the datetimestamp of transaction.\n",
      "score takes float datatype and represents the score of the transaction.\n",
      "Note that a customer can make multiple transactions for a product but the customer product pair will be just one entry for each order.\n",
      "Pandas DataFrame customer_dfcontaining customer data with columns user_id, customer_city.\n",
      "user_id takes string datatype and identifies the customer.\n",
      "customer_city takes string datatype and represents the city where the customer resides.\n",
      "Note that df can be joined with customer_df on user_id.\n",
      "Pandas DataFrame product_dfcontaining product data with columns item_id, product_category.\n",
      "item_id takes string datatype that identifies the product.\n",
      "product_category takes string datatype and represents the product category in the product hierarchy.\n",
      "Note that df can be joined with product_df on item_id.\n",
      "\n",
      "### Answer: \n",
      "def avg_transaction_score_per_category(df, customer_df, product_df):\\n    merged_df = pd.merge(df, customer_df, on='user_id')\\n    merged_df = pd.merge(merged_df, product_df, on='item_id')\\n    category_scores = merged_df.groupby('product_category')['score'].mean()\\n    return category_scores\\n\n",
      "\n",
      "### Explanation: \n",
      "This answer provides a Python function named 'avg_transaction_score_per_category' that calculates the average transaction score per product category. The function takes three arguments: 'df', which contains transaction data; 'customer_df', which contains customer data; and 'product_df', which contains product data. It first merges the transaction data with customer data using the 'user_id' column, then it merges the resulting DataFrame with product data using the 'item_id' column. Next, it groups the merged DataFrame by 'product_category' and computes the mean score within each group. Finally, it returns the result as a Pandas Series object.\\n\n"
     ]
    }
   ],
   "source": [
    "# Disable gradient calculation during inference\n",
    "with torch.no_grad():\n",
    "    # Generate text based on the model input\n",
    "    generated_tokens = ft_model.generate(\n",
    "        **model_input,                            # Pass model input\n",
    "        max_new_tokens=1024,                      # Maximum number of new tokens to generate\n",
    "        repetition_penalty=1.15,                  # Repetition penalty to avoid repetition\n",
    "        pad_token_id=eval_tokenizer.eos_token_id  # Set pad token ID\n",
    "    )[0]                                          # Get the first generated sequence\n",
    "\n",
    "    # Decode the generated tokens into text, skipping special tokens\n",
    "    generated_text = eval_tokenizer.decode(\n",
    "        generated_tokens,         # Generated tokens\n",
    "        skip_special_tokens=True  # Skip special tokens like padding and eos\n",
    "    )\n",
    "\n",
    "    # Print the generated text\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7b16652-b623-4047-bf28-25897ff9cf75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': '\\nYou are a Python function generator. Users will ask you questions in English, \\nand you will produce a Python function as answer to the question based on the provided CONTEXT.\\n\\nCONTEXT:\\nPandas DataFrame df containing transaction data with columns order_id, user_id, item_id, timestamp, score.\\r\\norder_id takes string datatype and identifies the order.\\r\\nuser_id takes string datatype and identifies the customer.\\r\\nitem_id takes string datatype that identifies the product.\\r\\ntimestamp takes timestamp datatype and represents the datetimestamp of transaction.\\r\\nscore takes float datatype and represents the score of the transaction.\\r\\nNote that a customer can make multiple transactions for a product but the customer product pair will be just one entry for each order.\\r\\nPandas DataFrame customer_dfcontaining customer data with columns user_id, customer_city.\\r\\nuser_id takes string datatype and identifies the customer.\\r\\ncustomer_city takes string datatype and represents the city where the customer resides.\\r\\nNote that df can be joined with customer_df on user_id.\\r\\nPandas DataFrame product_dfcontaining product data with columns item_id, product_category.\\r\\nitem_id takes string datatype that identifies the product.\\r\\nproduct_category takes string datatype and represents the product category in the product hierarchy.\\r\\nNote that df can be joined with product_df on item_id.\\n',\n",
       "   'role': 'system'},\n",
       "  {'content': 'What is the average transaction score for each product category?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'def customer_segmentation(df, customer_df, product_df, segmentation_strategy=\\'product_category\\'):\\\\n    \"\"\"\\\\n    Perform customer segmentation based on the given strategy.\\\\n    Parameters:\\\\n    df (DataFrame): Transaction data.\\\\n    customer_df (DataFrame): Customer data.\\\\n    product_df (DataFrame): Product data.\\\\n    segmentation_strategy (str): Strategy for segmentation.\\\\n                                Options: \\'demographics\\', \\'product_category\\', \\'transaction_behavior\\'\\\\n    Returns:\\\\n    DataFrame: Segmented customer groups with relevant information.\\\\n    \"\"\"\\\\n    merged_df = pd.merge(df, customer_df, on=\\'user_id\\')\\\\n    merged_df = pd.merge(merged_df, product_df, on=\\'item_id\\')\\\\n    if segmentation_strategy == \\'demographics\\':\\\\n        result = merged_df.groupby(\\'customer_city\\').agg({\\'score\\': \\'mean\\'}).reset_index()\\\\n        result.rename(columns={\\'score\\': \\'average_transaction_score\\'}, inplace=True)\\\\n    elif segmentation_strategy == \\'product_category\\':\\\\n        result = merged_df.groupby(\\'product_category\\').agg({\\'score\\': \\'mean\\'}).reset_index()\\\\n        result.rename(columns={\\'score\\': \\'average_transaction_score\\'}, inplace=True)\\\\n    elif segmentation_strategy == \\'transaction_behavior\\':\\\\n        result = merged_df.groupby(\\'user_id\\').agg({\\'order_id\\': \\'count\\', \\'score\\': \\'mean\\'}).reset_index()\\\\n        result.rename(columns={\\'order_id\\': \\'total_transactions\\', \\'score\\': \\'average_transaction_score\\'}, inplace=True)\\\\n    return result\\\\n',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_test_sample # Original test message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a231c-14a6-4651-b09c-5a3903a75e55",
   "metadata": {},
   "source": [
    "Retrieval Augmented Generation (RAG) indeed leverages semantic search to enhance text generation by retrieving relevant context from a large database. This approach ensures that generated text is more accurate and contextually appropriate. Using powerful VectorDBs can certainly scale up the performance of semantic search, making it more efficient and effective in handling large volumes of data. This combination of techniques holds a lot of promise for improving natural language generation tasks across various domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python new (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
