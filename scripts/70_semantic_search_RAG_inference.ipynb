{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20163a38-27a3-4f3d-8d71-8d3f7d16b630",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "https://www.sbert.net/examples/applications/semantic-search/README.html\n",
    "\n",
    "https://www.sbert.net/docs/pretrained_models.html\n",
    "\n",
    "https://paperswithcode.com/sota/code-generation-on-humaneval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3cc9ec-9d52-4561-9b33-1997f0fb9b83",
   "metadata": {},
   "source": [
    "In this notebook, I loaded datasets containing training, evaluation, and test samples for a question answering task. After extracting questions and contexts from the training and evaluation datasets, I utilized the SentenceTransformer library to load a pre-trained model for encoding text into embeddings. Using this model, I encoded the training and evaluation questions, normalized the embeddings, and employed semantic search to find the most similar question in the training set for each evaluation question. Subsequently, I calculated the accuracy of this model in identifying the correct context for evaluation questions and determined the best-performing model based on accuracy. Furthermore, I authenticated with the Hugging Face Hub and imported necessary libraries to work with a specific pre-trained model called Mistral AI `mistralai/Mistral-7B-v0.1`. After defining its configuration and loading its weights for causal language modeling, I loaded a previously trained and fine-tuned model from a checkpoint directory. To ensure reproducibility, I set a random seed and selected a random test sample from the test dataset. Then, I encoded the test query, found the most similar question in the training set, and retrieved its context. Using this information, I created a prompt and tokenized it for model input. Finally, I generated text based on the model input, disabled gradient calculation during inference, and decoded the generated text to produce a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e404e5-d8c1-493a-8e50-63140608d09b",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178e6420-6817-4874-9dc0-19756df8808d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load training dataset\n",
    "train_dataset = load_dataset('json', data_files='../data/train_CRM_data.json', split='train')\n",
    "\n",
    "# Load evaluation dataset\n",
    "eval_dataset = load_dataset('json', data_files='../data/val_CRM_data.json', split='train')\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = load_dataset('json', data_files='../data/test_CRM_data.json', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ce6a8-3553-4411-afc1-6f0010bd3311",
   "metadata": {},
   "source": [
    "# Extract Questions and Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aef2c56-ed65-48cd-8851-281d7b051df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_question_context_dict(message):\n",
    "    \"\"\"\n",
    "    Create a dictionary with question as key and context as value\n",
    "\n",
    "    Args:\n",
    "        message (list): A list containing dictionaries for question, context, and answer.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of question as key and context as value\n",
    "    \"\"\"\n",
    "    # Extracting question, context, and answer from the message\n",
    "    question = message[1]['content']\n",
    "    context = message[0]['content']\n",
    "    answer = message[2]['content']\n",
    "\n",
    "    return {question: context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62195c60-05f2-430c-b864-8e3f513f6392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_question_context_dict = {}\n",
    "\n",
    "# Iterate through each example in the training dataset\n",
    "for sample in train_dataset:\n",
    "    \n",
    "    # Extract question and context using create_question_context_dict function\n",
    "    qc_pair = create_question_context_dict(sample['messages'])\n",
    "    \n",
    "    # Merge the extracted pairs into the train_question_context_dict\n",
    "    train_question_context_dict.update(qc_pair)\n",
    "\n",
    "eval_question_context_dict = {}\n",
    "\n",
    "# Iterate through each example in the evaluation dataset\n",
    "for sample in eval_dataset:\n",
    "    \n",
    "    # Extract question and context using create_question_context_dict function\n",
    "    qc_pair = create_question_context_dict(sample['messages'])\n",
    "    \n",
    "    # Merge the extracted pairs into the eval_question_context_dict\n",
    "    eval_question_context_dict.update(qc_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05693c07-0788-4f94-b0c2-8b1de6c5c9b4",
   "metadata": {},
   "source": [
    "# Load SentenceTransformer Model, Encode Training Questions, Encode Evaluation Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2de1778-6f44-4889-bff6-ee7c904fb0e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Load pre-trained SentenceTransformer model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Extracting all questions from train_question_context_dict\n",
    "train_questions_corpus = list(train_question_context_dict.keys())\n",
    "\n",
    "# Encode the questions into embeddings\n",
    "train_corpus_embeddings = embedder.encode(train_questions_corpus, convert_to_tensor=True)\n",
    "\n",
    "# Move embeddings to GPU if available\n",
    "train_corpus_embeddings = train_corpus_embeddings.to(\"cuda\")\n",
    "\n",
    "# Normalize embeddings\n",
    "train_corpus_embeddings = util.normalize_embeddings(train_corpus_embeddings)\n",
    "\n",
    "# Query sentences:\n",
    "eval_questions_queries = list(eval_question_context_dict.keys())\n",
    "\n",
    "# Encode evaluation questions into embeddings\n",
    "query_embeddings = embedder.encode(eval_questions_queries, convert_to_tensor=True)\n",
    "\n",
    "# Move query embeddings to GPU if available\n",
    "query_embeddings = query_embeddings.to(\"cuda\")\n",
    "\n",
    "# Normalize query embeddings\n",
    "query_embeddings = util.normalize_embeddings(query_embeddings)\n",
    "\n",
    "# Find the most similar question in the training set for each evaluation question\n",
    "hits = util.semantic_search(query_embeddings, train_corpus_embeddings, score_function=util.dot_score, top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e984e2-804c-4397-b47c-4c6989f6851b",
   "metadata": {},
   "source": [
    "# Find Similar Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc223aad-eea1-4f14-8fdf-1e0cde89e775",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Eval Question  \\\n",
      "0   What is the correlation between transaction sc...   \n",
      "1   Most Common Product Category Mentioned in Cust...   \n",
      "2   Are there any products with a spike in transac...   \n",
      "3   Could you evaluate the engagement predictions ...   \n",
      "4   Can you identify any outliers in transaction s...   \n",
      "5   Are there any products with consistently low t...   \n",
      "6          Can you identify the top-selling products?   \n",
      "7   How many transactions have occurred for each c...   \n",
      "8   Are there any products with a consistent incre...   \n",
      "9   Are there any trends or patterns in purchasing...   \n",
      "10  Are there any specific customer segments that ...   \n",
      "11  How many transactions in our database have a s...   \n",
      "12  Are there any products that are frequently pur...   \n",
      "13  Are there any products with a spike in transac...   \n",
      "14  Are there any trends in the rankings of produc...   \n",
      "15  Are there any outliers in terms of high-volume...   \n",
      "16  Are there any external factors or market trend...   \n",
      "17  Can you identify any patterns in transaction v...   \n",
      "18  What is the average transaction score for tran...   \n",
      "19  Can you find the most common product category ...   \n",
      "20  Can you provide me with the next best offer re...   \n",
      "21  How does customer satisfaction and loyalty imp...   \n",
      "22  Are there any trends or patterns in purchasing...   \n",
      "23  I'm interested in forecasting the purchases of...   \n",
      "24  Are there any seasonal trends or events that i...   \n",
      "25  How many transactions have occurred for each c...   \n",
      "26  How frequently are customers from different ci...   \n",
      "27  How many unique customers have made transactio...   \n",
      "28  Can you provide a breakdown of transactions by...   \n",
      "29  Can you provide insights into the effectivenes...   \n",
      "30  Can you provide rankings of products across di...   \n",
      "31  What is the distribution of product categories...   \n",
      "32  What is the average transaction score for tran...   \n",
      "33  What is our current churn rate and cost per ac...   \n",
      "34  Can you identify any outliers or anomalies in ...   \n",
      "35  Can you identify any outliers or anomalies in ...   \n",
      "36  Are there any outliers in terms of high-volume...   \n",
      "37     What is the average time between transactions?   \n",
      "38  What is the average score for transactions in ...   \n",
      "39  Can you provide a summary of transactions by c...   \n",
      "40  How many transactions have occurred for each d...   \n",
      "\n",
      "                         Best Matching Train Question  \\\n",
      "0   What is the correlation between transaction sc...   \n",
      "1                          Popular Product Categories   \n",
      "2   Are there any products with a spike in transac...   \n",
      "3   What is the comparison between the predicted e...   \n",
      "4   Can you identify any outliers in transaction v...   \n",
      "5   Are there any products with consistently high ...   \n",
      "6   What are the top-selling products in each prod...   \n",
      "7   How many transactions have occurred for each c...   \n",
      "8   Are there any products with a consistent incre...   \n",
      "9   Are there any seasonal or temporal trends that...   \n",
      "10  Can you provide insights into the effectivenes...   \n",
      "11         What is the average score of transactions?   \n",
      "12  Are there any specific products that tend to b...   \n",
      "13  Are there any products with a spike in transac...   \n",
      "14  Are there any trends in the ratings of product...   \n",
      "15  Can you identify outliers among our customers ...   \n",
      "16  Are there any trends or patterns in customer b...   \n",
      "17  Can you identify any patterns in transaction v...   \n",
      "18  What is the average transaction score for tran...   \n",
      "19  What is the most common product category purch...   \n",
      "20  Can you provide the next best offer recommenda...   \n",
      "21  How does customer engagement and interaction i...   \n",
      "22  Are there any external factors or market trend...   \n",
      "23  Can you provide a prediction of the top-rated ...   \n",
      "24  Are there any seasonal trends or events that i...   \n",
      "25  How many transactions have occurred for each c...   \n",
      "26  How does the frequency of purchases vary betwe...   \n",
      "27  How many transactions have occurred for each p...   \n",
      "28  Can you provide a summary of transactions by p...   \n",
      "29  Can you identify any trends or correlations be...   \n",
      "30  Can you provide the average score of transacti...   \n",
      "31  How many transactions have occurred for each p...   \n",
      "32  What is the average transaction score for tran...   \n",
      "33  Could you analyze the churn predictions for ou...   \n",
      "34  Are there any trends or patterns in search beh...   \n",
      "35  How do the engagement likelihood predictions a...   \n",
      "36  Can you identify outliers among our customers ...   \n",
      "37  What is the average time between transactions ...   \n",
      "38  What is the average transaction value for each...   \n",
      "39  Can you provide a summary of transactions by c...   \n",
      "40  How many transactions have occurred for each p...   \n",
      "\n",
      "    Correct Context Identified  \n",
      "0                         True  \n",
      "1                        False  \n",
      "2                         True  \n",
      "3                         True  \n",
      "4                         True  \n",
      "5                        False  \n",
      "6                         True  \n",
      "7                         True  \n",
      "8                         True  \n",
      "9                         True  \n",
      "10                        True  \n",
      "11                       False  \n",
      "12                       False  \n",
      "13                       False  \n",
      "14                       False  \n",
      "15                        True  \n",
      "16                        True  \n",
      "17                        True  \n",
      "18                       False  \n",
      "19                        True  \n",
      "20                        True  \n",
      "21                        True  \n",
      "22                        True  \n",
      "23                       False  \n",
      "24                       False  \n",
      "25                        True  \n",
      "26                        True  \n",
      "27                        True  \n",
      "28                        True  \n",
      "29                        True  \n",
      "30                        True  \n",
      "31                        True  \n",
      "32                        True  \n",
      "33                        True  \n",
      "34                        True  \n",
      "35                       False  \n",
      "36                       False  \n",
      "37                       False  \n",
      "38                        True  \n",
      "39                       False  \n",
      "40                       False  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_data = []\n",
    "\n",
    "# Iterate through each evaluation question and its corresponding hit\n",
    "for idx, eval_questions_query in enumerate(eval_questions_queries):\n",
    "    eval_question = eval_questions_query\n",
    "    \n",
    "    # Get the best matching train question using the hits\n",
    "    best_matching_train_question = train_questions_corpus[hits[idx][0]['corpus_id']]\n",
    "    \n",
    "    # Check if the correct context is identified by comparing the contexts of the best matching train question and evaluation question\n",
    "    correct_context_identified = train_question_context_dict[best_matching_train_question] == eval_question_context_dict[eval_questions_query]\n",
    "    \n",
    "    # Append the data to the output list\n",
    "    output_data.append({\n",
    "        'Eval Question': eval_question,\n",
    "        'Best Matching Train Question': best_matching_train_question,\n",
    "        'Correct Context Identified': correct_context_identified\n",
    "    })\n",
    "\n",
    "# Creating a DataFrame from the output data\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Displaying the DataFrame\n",
    "print(output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9ee238-c17e-435c-a309-696d40fcdb5c",
   "metadata": {},
   "source": [
    "# Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd7218d-156f-486e-94e7-35fa8835937d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6585365853658537\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of evaluations\n",
    "total_evaluations = len(output_df)\n",
    "\n",
    "# Calculate the number of correct context identifications\n",
    "correct_identifications = output_df['Correct Context Identified'].sum()\n",
    "\n",
    "# Calculate accuracy by dividing the number of correct identifications by the total evaluations\n",
    "accuracy = correct_identifications / total_evaluations\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced16ce-ad00-4e3a-8d25-b99a4398bce4",
   "metadata": {},
   "source": [
    "# Identify Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd20db4f-b0e2-46dc-9c75-d9c641e18477",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6278f1e0f5470cb83fa22f7a1d7c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e151de6eedde47379d87b013dcd78a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21d767514b34b9888da08d9e7999b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b510d646ae4dec8d36da3d6575b27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8121ce1279941b2b7fc84f506c858c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a20d33d8434bf3bd3d89780d79005d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1504da9eb44fdaab2abcbff185b5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e3f3577bed49cd8625daac5c230ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc26d7ed7fd14e948f45c4301f0bc397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3801c0d298e3471794c618c626827342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a094771f5945eebe3b5d33f6a68fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e649e6ebb40846d0ad8d65acb77aef4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92c4b2c732246a2a5c071a226ad475b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2793d8c2ba94757914ec2afafd51022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f927e64c264c65a8e20881178ebfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272fc04399204d9a95feb7aa0acd60a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15058e9fd3a4e1f97f08c0d2a2bd9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/9.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acf8b47dca04319b559bb6df6635962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5565e7e28d4f34ae3ed8ad86f9511d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bb9957b8814e49a9ceba4b84aa2417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9b7bb3ef8e411382bd205c2807c57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01659e7455b484f85aba76824c61611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2a99b2fa134853b5bbf7d3d13cf6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6be741420a84eec9ed1ce214ad16909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/model.safetensors:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ff72790f2d47e2be55a74521f4ec20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fbfa4b3807496eaea6b017c4d50672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a63216c9bd84266a83543c73518044a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca78d68502b845a78937d6e524167fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e58dba814441f58bf78d3b6d5ccb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012184cc570440b180c3e26541e11c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15708c6ebf641e79551a36927df4ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7802fbd3544326baf834766badebbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c86beb61a954901a57a845f9e3080b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a400cac6df644308bef42e3596f02f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd43a77f28a8409796fdd6197716acf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91580749a8f48ad85d8c4ac91b1d5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ec6fb3c65b4cbe96ed5d64e3f4061e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d821e236afa64b51af43d40248744f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/model.safetensors:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a9036333cb4db585a8db9dff5d593a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52da2a51af9c42bbaeb4f8a5df22a21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b056e99a3a46e0a3d841f990179fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1451105aa444d2fa70905b13acf92f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c64fdea37ad43b586e4673b4daf18d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7e8f7c44284a23ae2c72de42b9465c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f216a2ae1e8f4f629e773d14dd567bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/9.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e334542c7049e0a6815647760acb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4ba32d4ae84dd1a1a7d88fbd250993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f55f72be294cd989869a55e1c19baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d259e6a7700c4f28aa113cefbf395fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394eef6ee181492cb04154d7904e84a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4758120786474f94b5ccfc4d3f72e9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e1d5d23ffd40178041fb60dc661763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1252dd1ff5b14d6da5d110caede34261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/model.safetensors:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb2975a96ac4d6ea534504b2a1baeb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf189f2d5574432b839ef1a72a111fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d622b2749a46369830ebd038fd1060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052a8b1611414d68b448742ccd65a888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096b8b1553f64ea38e54c152c398f334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1006: UserWarning: Not enough free disk space to download the file. The expected file size is: 669.90 MB. The target location /home/jovyan/.cache/huggingface/hub only has 496.44 MB free disk space.\n",
      "  warnings.warn(\n",
      "/home/jovyan/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1006: UserWarning: Not enough free disk space to download the file. The expected file size is: 669.90 MB. The target location /home/jovyan/.cache/huggingface/hub/models--sentence-transformers--gtr-t5-large/blobs only has 496.44 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a1e9c2c932491d8ce423fdbcced655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Evaluate each model and store accuracy results\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m model_names:\n\u001b[0;32m---> 95\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_question_context_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_question_context_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     accuracy_results[model_name] \u001b[38;5;241m=\u001b[39m accuracy\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Print accuracy results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mevaluate_model_accuracy\u001b[0;34m(model_name, train_question_context_dict, eval_question_context_dict)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model_accuracy\u001b[39m(model_name, train_question_context_dict, eval_question_context_dict):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     embedder \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Extracting all questions from train_question_context_dict\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     train_questions_corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(train_question_context_dict\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:197\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, cache_folder, trust_remote_code, revision, token, use_auth_token, truncate_dim)\u001b[0m\n\u001b[1;32m    194\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(model_name_or_path, token, cache_folder\u001b[38;5;241m=\u001b[39mcache_folder, revision\u001b[38;5;241m=\u001b[39mrevision):\n\u001b[0;32m--> 197\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    206\u001b[0m         model_name_or_path,\n\u001b[1;32m    207\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m    211\u001b[0m     )\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1296\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1295\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_args\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hub_kwargs\n\u001b[0;32m-> 1296\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;66;03m# Normalize does not require any files to be loaded\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_class \u001b[38;5;241m==\u001b[39m Normalize:\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:36\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case \u001b[38;5;241m=\u001b[39m do_lower_case\n\u001b[1;32m     35\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     39\u001b[0m     tokenizer_name_or_path \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model_name_or_path,\n\u001b[1;32m     40\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_args,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# No max_seq_length set. Try to infer from model\u001b[39;00m\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:61\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, **model_args)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads the transformer model\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, T5Config):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_t5_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, MT5Config):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:74\u001b[0m, in \u001b[0;36mTransformer._load_t5_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, **model_args)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5EncoderModel\n\u001b[1;32m     73\u001b[0m T5EncoderModel\u001b[38;5;241m.\u001b[39m_keys_to_ignore_on_load_unexpected \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder.*\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mT5EncoderModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3251\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3237\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   3238\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   3240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   3250\u001b[0m     }\n\u001b[0;32m-> 3251\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3253\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   3254\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   3255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   3256\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:389\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1492\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1490\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[0;32m-> 1492\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1503\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:550\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n\u001b[0;32m--> 550\u001b[0m     \u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     new_resume_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;66;03m# Some data has been downloaded from the server so we reset the number of retries.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/tempfile.py:483\u001b[0m, in \u001b[0;36m_TemporaryFileWrapper.__getattr__.<locals>.func_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;129m@_functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_model_accuracy(model_name, train_question_context_dict, eval_question_context_dict):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of a sentence transformer model on a given set of training and evaluation question-context pairs.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the sentence transformer model to be used for embedding.\n",
    "        train_question_context_dict (dict): Dictionary mapping training questions to their corresponding contexts.\n",
    "        eval_question_context_dict (dict): Dictionary mapping evaluation questions to their corresponding contexts.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model in identifying the correct context for evaluation questions.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "\n",
    "    # Extracting all questions from train_question_context_dict\n",
    "    train_questions_corpus = list(train_question_context_dict.keys())\n",
    "    \n",
    "    # Encode the training questions into embeddings and move them to GPU if available\n",
    "    train_corpus_embeddings = embedder.encode(train_questions_corpus, convert_to_tensor=True).to(\"cuda\")\n",
    "    \n",
    "    # Normalize the embeddings\n",
    "    train_corpus_embeddings = util.normalize_embeddings(train_corpus_embeddings)\n",
    "\n",
    "    # Query sentences:\n",
    "    eval_questions_queries = list(eval_question_context_dict.keys())\n",
    "    \n",
    "    # Encode the evaluation questions into embeddings and move them to GPU if available\n",
    "    query_embeddings = embedder.encode(eval_questions_queries, convert_to_tensor=True).to(\"cuda\")\n",
    "    \n",
    "    # Normalize the embeddings\n",
    "    query_embeddings = util.normalize_embeddings(query_embeddings)\n",
    "\n",
    "    # Find the most similar question in the training set for each evaluation question\n",
    "    hits = util.semantic_search(query_embeddings, train_corpus_embeddings, score_function=util.dot_score, top_k=1)\n",
    "\n",
    "    output_data = []\n",
    "    for idx, eval_questions_query in enumerate(eval_questions_queries):\n",
    "        eval_question = eval_questions_query\n",
    "        \n",
    "        # Get the best matching train question using the hits\n",
    "        best_matching_train_question = train_questions_corpus[hits[idx][0]['corpus_id']]\n",
    "        \n",
    "        # Check if the correct context is identified by comparing the contexts of the best matching train question and evaluation question\n",
    "        correct_context_identified = train_question_context_dict[best_matching_train_question] == eval_question_context_dict[eval_questions_query]\n",
    "\n",
    "        output_data.append({\n",
    "            'Eval Question': eval_question,\n",
    "            'Best Matching Train Question': best_matching_train_question,\n",
    "            'Correct Context Identified': correct_context_identified\n",
    "        })\n",
    "\n",
    "    # Creating a DataFrame from the output data\n",
    "    output_df = pd.DataFrame(output_data)\n",
    "\n",
    "    # Calculate the total number of evaluations\n",
    "    total_evaluations = len(output_df)\n",
    "\n",
    "    # Calculate the number of correct context identifications\n",
    "    correct_identifications = output_df['Correct Context Identified'].sum()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_identifications / total_evaluations\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# List of models to evaluate\n",
    "model_names = [\n",
    "    \"all-mpnet-base-v2\",\n",
    "    \"gtr-t5-xxl\", \n",
    "    \"gtr-t5-xl\", \n",
    "    \"sentence-t5-xxl\",\n",
    "    \"gtr-t5-large\",\n",
    "    \"all-mpnet-base-v1\",\n",
    "    \"multi-qa-mpnet-base-dot-v1\",\n",
    "    \"multi-qa-mpnet-base-cos-v1\",\n",
    "    \"all-roberta-large-v1\",\n",
    "    \"sentence-t5-xl\",\n",
    "    \"all-distilroberta-v1\",\n",
    "    \"all-MiniLM-L12-v1\",\n",
    "    \"all-MiniLM-L12-v2\",\n",
    "    \"multi-qa-distilbert-dot-v1\",\n",
    "    \"multi-qa-distilbert-cos-v1\",\n",
    "    \"gtr-t5-base\",\n",
    "    \"sentence-t5-large\",\n",
    "    \"all-MiniLM-L6-v2\",\n",
    "    \"multi-qa-MiniLM-L6-cos-v1\",\n",
    "    \"all-MiniLM-L6-v1\",\n",
    "    \"paraphrase-mpnet-base-v2\",\n",
    "    \"msmarco-bert-base-dot-v5\",\n",
    "    \"multi-qa-MiniLM-L6-dot-v1\",\n",
    "    \"sentence-t5-base\",\n",
    "    \"msmarco-distilbert-base-tas-b\",\n",
    "    \"msmarco-distilbert-dot-v5\",\n",
    "    \"paraphrase-distilroberta-base-v2\",\n",
    "    \"paraphrase-MiniLM-L12-v2\",\n",
    "    \"paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"paraphrase-TinyBERT-L6-v2\",\n",
    "    \"paraphrase-MiniLM-L6-v2\",\n",
    "    \"paraphrase-albert-small-v2\",\n",
    "    \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"paraphrase-MiniLM-L3-v2\",\n",
    "    \"distiluse-base-multilingual-cased-v1\",\n",
    "    \"distiluse-base-multilingual-cased-v2\",\n",
    "    \"average_word_embeddings_komninos\",\n",
    "    \"average_word_embeddings_glove.6B.300d\"\n",
    "]\n",
    "\n",
    "# Dictionary to store accuracy results\n",
    "accuracy_results = {}\n",
    "\n",
    "# Evaluate each model and store accuracy results\n",
    "for model_name in model_names:\n",
    "    accuracy = evaluate_model_accuracy(model_name, train_question_context_dict, eval_question_context_dict)\n",
    "    accuracy_results[model_name] = accuracy\n",
    "\n",
    "# Print accuracy results\n",
    "for model_name, accuracy in accuracy_results.items():\n",
    "    print(f\"Model: {model_name}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe5a3da7-8d4c-480b-82bf-4a5eaa11c23b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model based on accuracy is 'all-mpnet-base-v2' with an accuracy of 73.17%.\n",
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# Find the model with the highest accuracy\n",
    "best_model = max(accuracy_results, key=accuracy_results.get)\n",
    "\n",
    "# Get the accuracy of the best model\n",
    "best_accuracy = accuracy_results[best_model]\n",
    "\n",
    "# Print the best model and its accuracy\n",
    "print(f\"The best model based on accuracy is '{best_model}' with an accuracy of {best_accuracy:.2%}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620ed08-e5de-4e47-a945-63f3e90b2c71",
   "metadata": {},
   "source": [
    "# Authenticate Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432a3393-0394-49ff-acf4-b2b9a4058ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93d27ccd06e406ebee6b08ab911ae43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the notebook_login function from huggingface_hub module\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Call the notebook_login function to authenticate\n",
    "# Provide an access token from the provided URL - https://huggingface.co/settings/tokens\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39cb892-4932-437d-b4f4-46564968f743",
   "metadata": {},
   "source": [
    "# Import Libraries and Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e0104a-cbfa-42a5-9b33-4d7b2b9bf1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f9359ab753490c843bccd902a2e282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# Define Mistral's pretrained model ID\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# Define BitsAndBytesConfig for Mistral's model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Load the model in 4-bit precision\n",
    "    bnb_4bit_use_double_quant=True,       # Use double quantization for 4-bit quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Use nf4 quantization for 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 for computation with 4-bit quantization\n",
    ")\n",
    "\n",
    "# Load Mistral's pretrained model for causal language modeling\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id, \n",
    "    quantization_config=bnb_config, # Apply the defined quantization configuration\n",
    "    device_map=\"auto\"               # Automatically select the device for model inference\n",
    ")\n",
    "\n",
    "# Load Mistral's tokenizer\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id, \n",
    "    add_bos_token=True,     # Add beginning-of-sequence token\n",
    "    trust_remote_code=True  # Trust remote code for tokenization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ebef3-bc8a-41dc-a9a8-5c0b2e101a32",
   "metadata": {},
   "source": [
    "# Load Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42053b98-7f93-42d9-adce-959728cbf4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load the weights from the checkpoint directory\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"../models/01-finetune-mistral/checkpoint-150\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8887ea6-89ef-4c8b-9b39-e0f980c7ad52",
   "metadata": {},
   "source": [
    "# Retrieve context for a random test sample question and prepare prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98338e12-ee76-43a2-a329-87d6aef023df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): lora.Linear(\n",
       "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=64, out_features=32000, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Take a random index\n",
    "random_index = random.randint(0, len(test_dataset) - 1)\n",
    "\n",
    "# Take the random sample\n",
    "random_test_sample = test_dataset[random_index]\n",
    "\n",
    "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Extracting all questions from train_question_context_dict\n",
    "train_questions_corpus = list(train_question_context_dict.keys())\n",
    "\n",
    "# Encode the training questions into embeddings and move them to GPU if available\n",
    "train_corpus_embeddings = embedder.encode(train_questions_corpus, convert_to_tensor=True).to(\"cuda\")\n",
    "\n",
    "# Normalize the embeddings\n",
    "train_corpus_embeddings = util.normalize_embeddings(train_corpus_embeddings)\n",
    "\n",
    "# Query sentence:\n",
    "test_query = [random_test_sample['messages'][1]['content']]\n",
    "\n",
    "# Encode the test query into embeddings and move them to GPU if available\n",
    "query_embedding = embedder.encode(test_query, convert_to_tensor=True).to(\"cuda\")\n",
    "\n",
    "# Normalize the embeddings\n",
    "query_embedding = util.normalize_embeddings(query_embedding)\n",
    "\n",
    "# Find the most similar question in the training set for the test query\n",
    "hits = util.semantic_search(query_embedding, train_corpus_embeddings, score_function=util.dot_score, top_k=1)\n",
    "\n",
    "# Retrieve the context corresponding to the most similar question\n",
    "retrieved_context = train_question_context_dict[train_questions_corpus[hits[0][0]['corpus_id']]]\n",
    "\n",
    "# Create a prompt with the test query and the retrieved context\n",
    "prompt = f\"\"\"\n",
    "### Question: {test_query[0]}\n",
    "\n",
    "### Context: {retrieved_context}\n",
    "### Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "# Init a tokenizer that doesn't add padding or eos token\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "# Tokenize the test prompt and prepare model input for inference\n",
    "model_input = test_tokenizer(\n",
    "    prompt, \n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")  # Move tensors to GPU if available\n",
    "\n",
    "# Set the fine-tuned model to evaluation mode\n",
    "ft_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1a467-72b9-4b2c-8a6d-bb43e61981ba",
   "metadata": {},
   "source": [
    "# Model Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4922a444-0c62-4f7c-bc35-f5008ab1bc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:05:56.766195: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-06 13:05:56.824131: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-06 13:05:57.652431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: What is the average transaction score for each product category?\n",
      "\n",
      "### Context: \n",
      "You are a Python function generator. Users will ask you questions in English, \n",
      "and you will produce a Python function as answer to the question based on the provided CONTEXT.\n",
      "\n",
      "CONTEXT:\n",
      "Pandas DataFrame df containing transaction data with columns order_id, user_id, item_id, timestamp, score.\n",
      "order_id takes string datatype and identifies the order.\n",
      "user_id takes string datatype and identifies the customer.\n",
      "item_id takes string datatype that identifies the product.\n",
      "timestamp takes timestamp datatype and represents the datetimestamp of transaction.\n",
      "score takes float datatype and represents the score of the transaction.\n",
      "Note that a customer can make multiple transactions for a product but the customer product pair will be just one entry for each order.\n",
      "Pandas DataFrame customer_dfcontaining customer data with columns user_id, customer_city.\n",
      "user_id takes string datatype and identifies the customer.\n",
      "customer_city takes string datatype and represents the city where the customer resides.\n",
      "Note that df can be joined with customer_df on user_id.\n",
      "Pandas DataFrame product_dfcontaining product data with columns item_id, product_category.\n",
      "item_id takes string datatype that identifies the product.\n",
      "product_category takes string datatype and represents the product category in the product hierarchy.\n",
      "Note that df can be joined with product_df on item_id.\n",
      "\n",
      "### Answer: \n",
      "def avg_transaction_score_per_category(df, customer_df, product_df):\\n    merged_df = pd.merge(df, customer_df, on='user_id')\\n    merged_df = pd.merge(merged_df, product_df, on='item_id')\\n    category_scores = merged_df.groupby('product_category')['score'].mean()\\n    return category_scores\\n\n",
      "\n",
      "### Explanation: \n",
      "This answer provides a Python function named 'avg_transaction_score_per_category' that calculates the average transaction score per product category. The function takes three arguments: 'df', which contains transaction data; 'customer_df', which contains customer data; and 'product_df', which contains product data. It first merges the transaction data with customer data using the 'user_id' column, then it merges the resulting DataFrame with product data using the 'item_id' column. Next, it groups the merged DataFrame by 'product_category' and computes the mean score within each group. Finally, it returns the result as a Pandas Series object.\\n\n"
     ]
    }
   ],
   "source": [
    "# Disable gradient calculation during inference\n",
    "with torch.no_grad():\n",
    "    # Generate text based on the model input\n",
    "    generated_tokens = ft_model.generate(\n",
    "        **model_input,                            # Pass model input\n",
    "        max_new_tokens=1024,                      # Maximum number of new tokens to generate\n",
    "        repetition_penalty=1.15,                  # Repetition penalty to avoid repetition\n",
    "        pad_token_id=eval_tokenizer.eos_token_id  # Set pad token ID\n",
    "    )[0]                                          # Get the first generated sequence\n",
    "\n",
    "    # Decode the generated tokens into text, skipping special tokens\n",
    "    generated_text = eval_tokenizer.decode(\n",
    "        generated_tokens,         # Generated tokens\n",
    "        skip_special_tokens=True  # Skip special tokens like padding and eos\n",
    "    )\n",
    "\n",
    "    # Print the generated text\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a231c-14a6-4651-b09c-5a3903a75e55",
   "metadata": {},
   "source": [
    "Retrieval Augmented Generation (RAG) indeed leverages semantic search to enhance text generation by retrieving relevant context from a large database. This approach ensures that generated text is more accurate and contextually appropriate. Using powerful VectorDBs can certainly scale up the performance of semantic search, making it more efficient and effective in handling large volumes of data. This combination of techniques holds a lot of promise for improving natural language generation tasks across various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba601aac-dd4c-4a0b-b051-1a498a924ebe",
   "metadata": {},
   "source": [
    "# Extracting Answer Section from Generated Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12513504-2e3a-4ec9-b0f5-6f9a292c1668",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def avg_transaction_score_per_category(df, customer_df, product_df):\n",
      "    merged_df = pd.merge(df, customer_df, on='user_id')\n",
      "    merged_df = pd.merge(merged_df, product_df, on='item_id')\n",
      "    category_scores = merged_df.groupby('product_category')['score'].mean()\n",
      "    return category_scores\n"
     ]
    }
   ],
   "source": [
    "# Find the start position of the answer section\n",
    "start_marker = \"### Answer:\"\n",
    "start_index = generated_text.find(start_marker) + len(start_marker)\n",
    "\n",
    "# If the start marker is found\n",
    "if start_index != -1:\n",
    "    # Extract the answer\n",
    "    answer = generated_text[start_index:].strip()\n",
    "    # Check if there is an explanation part\n",
    "    if \"### Explanation:\" in answer:\n",
    "        # Find the end position of the answer section\n",
    "        end_marker = \"### Explanation:\"\n",
    "        end_index = answer.find(end_marker)\n",
    "        # Extract only the answer part\n",
    "        answer = answer[:end_index].strip()\n",
    "else:\n",
    "    answer = \"No answer found.\"\n",
    "\n",
    "answer = answer.replace(\"\\\\\\\\\", \"\\\\\").strip().replace(\"\\\\n\", \"\\n\").rstrip('\\\\').replace(\"\\\\'\", \"'\").strip()\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc3c69a-95ee-4f89-bb2c-b4b4a0b64d94",
   "metadata": {},
   "source": [
    "# Parsing Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f418b4-4f23-471d-b41b-bb766cc0da75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_transaction_score_per_category\n",
      "['df', 'customer_df', 'product_df']\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from numpy.linalg import LinAlgError\n",
      "from datetime import datetime, timedelta\n",
      "from collections import defaultdict, Counter\n",
      "from itertools import combinations\n",
      "from scipy.sparse import csr_matrix\n",
      "from scipy.stats import zscore\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
      "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import association_rules\n",
      "from surprise import Reader, Dataset\n",
      "from surprise.prediction_algorithms import SVD\n",
      "from matplotlib import pyplot as plt\n",
      "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
      "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "import warnings\n",
      "def avg_transaction_score_per_category(df, customer_df, product_df):\n",
      "    merged_df = pd.merge(df, customer_df, on='user_id')\n",
      "    merged_df = pd.merge(merged_df, product_df, on='item_id')\n",
      "    category_scores = merged_df.groupby('product_category')['score'].mean()\n",
      "    return category_scores\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import astor\n",
    "\n",
    "# Function to parse function definition string\n",
    "def parse_function_definition(definition_string):\n",
    "    \"\"\"\n",
    "    This function parses a given function definition string and extracts important information such as function name, arguments, and body statements. It then constructs a complete function definition string by adding necessary imports and the original function definition.\n",
    "\n",
    "    Parameters:\n",
    "    - `definition_string`: The string representing the function definition.\n",
    "\n",
    "    Returns:\n",
    "    - `function_name`: Name of the parsed function.\n",
    "    - `arguments`: List of arguments of the parsed function.\n",
    "    - `body`: Body statements of the parsed function.\n",
    "    - `function_definition`: Complete function definition string including necessary imports and the original function definition.\n",
    "    \"\"\"\n",
    "    # Parse the definition string\n",
    "    parsed = ast.parse(definition_string)\n",
    "    \n",
    "    # Initialize variables\n",
    "    function_name = None\n",
    "    arguments = []\n",
    "    body = []\n",
    "    \n",
    "    # Iterate over parsed body\n",
    "    for node in parsed.body:\n",
    "        # Check if node is a function definition\n",
    "        if isinstance(node, ast.FunctionDef):\n",
    "            # Get function name\n",
    "            function_name = node.name\n",
    "            # Get function arguments\n",
    "            arguments = [arg.arg for arg in node.args.args]\n",
    "            \n",
    "            # Iterate over function body statements\n",
    "            for stmt in node.body:\n",
    "                # Convert AST node to source code and append to body list\n",
    "                body.append(astor.to_source(stmt).strip())\n",
    "    \n",
    "    # Construct complete function definition string\n",
    "    function_definition = (\n",
    "        \"\\n\"\n",
    "        # Import necessary modules\n",
    "        f\"import pandas as pd\\n\"\n",
    "        f\"import numpy as np\\n\"\n",
    "        f\"from numpy.linalg import LinAlgError\\n\"\n",
    "        f\"from datetime import datetime, timedelta\\n\"\n",
    "        f\"from collections import defaultdict, Counter\\n\"\n",
    "        f\"from itertools import combinations\\n\"\n",
    "        f\"from scipy.sparse import csr_matrix\\n\"\n",
    "        f\"from scipy.stats import zscore\\n\"\n",
    "        f\"from sklearn.preprocessing import StandardScaler, LabelEncoder\\n\"\n",
    "        f\"from sklearn.cluster import KMeans\\n\"\n",
    "        f\"from sklearn.model_selection import train_test_split\\n\"\n",
    "        f\"from sklearn.linear_model import LogisticRegression, LinearRegression\\n\"\n",
    "        f\"from sklearn.metrics.pairwise import cosine_similarity\\n\"\n",
    "        f\"from sklearn.feature_extraction.text import TfidfVectorizer\\n\"\n",
    "        f\"from sklearn.decomposition import TruncatedSVD\\n\"\n",
    "        f\"from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\\n\"\n",
    "        f\"from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\\n\"\n",
    "        f\"from mlxtend.frequent_patterns import apriori\\n\"\n",
    "        f\"from mlxtend.frequent_patterns import association_rules\\n\"\n",
    "        f\"from surprise import Reader, Dataset\\n\"\n",
    "        f\"from surprise.prediction_algorithms import SVD\\n\"\n",
    "        f\"from matplotlib import pyplot as plt\\n\"\n",
    "        f\"from statsmodels.tsa.statespace.sarimax import SARIMAX\\n\"\n",
    "        f\"from statsmodels.tools.sm_exceptions import ConvergenceWarning\\n\"\n",
    "        f\"from statsmodels.tsa.seasonal import seasonal_decompose\\n\"\n",
    "        f\"import warnings\\n\"\n",
    "        # Append original function definition\n",
    "        f\"{definition_string}\"\n",
    "    )\n",
    "    return function_name, arguments, '\\n'.join(body), function_definition\n",
    "\n",
    "# Parse the function definition and retrieve function name, arguments, and complete function definition\n",
    "function_name, arguments, _, function_definition = parse_function_definition(answer)\n",
    "\n",
    "# Print function name, arguments, and complete function definition\n",
    "print(function_name)\n",
    "print(arguments)\n",
    "print(function_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a5aaf4-7a8f-4261-8ae8-99973ab73704",
   "metadata": {},
   "source": [
    "# Executing Parsed Function Definition with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cf50293-9af7-4bc4-afe0-ac380d45138a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders Data:\n",
      "   order_id user_id item_id                  timestamp  score\n",
      "0    order1   user1   item1 2024-05-03 09:35:07.574568   0.87\n",
      "1    order2   user2   item1 2024-05-05 18:32:24.651137   0.57\n",
      "2    order3   user1   item3 2024-05-04 08:18:30.474318   0.87\n",
      "3    order4   user5   item1 2024-05-06 02:07:19.433402   0.80\n",
      "4    order5   user1   item1 2024-05-02 07:25:38.185993   0.55\n",
      "5    order6   user2   item3 2024-05-03 08:24:52.126336   0.80\n",
      "6    order7   user5   item1 2024-05-06 05:22:24.670369   0.86\n",
      "7    order8   user5   item2 2024-05-04 20:40:11.943480   0.61\n",
      "8    order9   user5   item2 2024-05-05 01:39:06.918721   0.90\n",
      "9   order10   user1   item1 2024-05-04 03:06:28.890031   0.85\n",
      "10  order11   user3   item2 2024-05-03 00:16:38.741952   0.58\n",
      "11  order12   user3   item1 2024-05-03 16:46:54.816524   0.55\n",
      "12  order13   user1   item2 2024-05-05 02:16:28.347155   0.92\n",
      "13  order14   user5   item2 2024-05-02 08:43:39.155055   0.90\n",
      "14  order15   user4   item3 2024-05-06 09:07:40.112240   0.56\n",
      "\n",
      "Customer Data:\n",
      "  user_id customer_city\n",
      "0   user1         Paris\n",
      "1   user2        London\n",
      "2   user3         Paris\n",
      "3   user4         Paris\n",
      "4   user5        London\n",
      "\n",
      "Product Data:\n",
      "  item_id product_category\n",
      "0   item1          himself\n",
      "1   item2       production\n",
      "2   item3        represent\n",
      "3   item4        treatment\n",
      "4   item5         identify\n",
      "\n",
      "Answer to user query:\n",
      "product_category\n",
      "himself       0.721429\n",
      "production    0.782000\n",
      "represent     0.743333\n",
      "Name: score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "import inspect\n",
    "\n",
    "# Initialize Faker for generating fake data\n",
    "fake = Faker()\n",
    "\n",
    "# List of famous city names\n",
    "european_cities = [\n",
    "    'London', 'Paris'\n",
    "]\n",
    "\n",
    "# Generate sample data for orders\n",
    "orders_data = []\n",
    "for i in range(1, 16):\n",
    "    order_id = f'order{i}'\n",
    "    user_id = f'user{random.randint(1, 5)}'\n",
    "    item_id = f'item{random.randint(1, 3)}'\n",
    "    timestamp = fake.date_time_between(start_date='-5d', end_date='now')\n",
    "    score = round(random.uniform(0.5, 1.0), 2)\n",
    "    orders_data.append({'order_id': order_id, 'user_id': user_id, 'item_id': item_id, 'timestamp': timestamp, 'score': score})\n",
    "orig_df = pd.DataFrame(orders_data)\n",
    "\n",
    "# Generate sample data for customers\n",
    "customers_data = []\n",
    "for i in range(1, 6):\n",
    "    user_id = f'user{i}'\n",
    "    customer_city = random.choice(european_cities)\n",
    "    customers_data.append({'user_id': user_id, 'customer_city': customer_city})\n",
    "orig_customer_df = pd.DataFrame(customers_data).drop_duplicates(subset=['user_id'])\n",
    "\n",
    "# Generate sample data for products\n",
    "products_data = []\n",
    "for i in range(1, 6):\n",
    "    item_id = f'item{i}'\n",
    "    product_category = fake.word()\n",
    "    products_data.append({'item_id': item_id, 'product_category': product_category})\n",
    "orig_product_df = pd.DataFrame(products_data).drop_duplicates(subset=['item_id'])\n",
    "\n",
    "# Function to copy original data frames\n",
    "def data_copy(orig_df, orig_customer_df, orig_product_df):\n",
    "    \"\"\"\n",
    "    This function creates deep copies of the original data frames to prevent modification of the original data.\n",
    "\n",
    "    Parameters:\n",
    "    - `orig_df`: Original DataFrame containing transaction data.\n",
    "    - `orig_customer_df`: Original DataFrame containing customer data.\n",
    "    - `orig_product_df`: Original DataFrame containing product data.\n",
    "\n",
    "    Returns:\n",
    "    - `df`: Deep copy of the original transaction DataFrame.\n",
    "    - `customer_df`: Deep copy of the original customer DataFrame.\n",
    "    - `product_df`: Deep copy of the original product DataFrame.\n",
    "    \"\"\"\n",
    "    df = orig_df.copy(deep=True)\n",
    "    customer_df = orig_customer_df.copy(deep=True)\n",
    "    product_df = orig_product_df.copy(deep=True)\n",
    "    return df, customer_df, product_df\n",
    "\n",
    "# Copy original data frames\n",
    "df, customer_df, product_df = data_copy(orig_df, orig_customer_df, orig_product_df)\n",
    "\n",
    "# Displaying sample data\n",
    "print(\"Orders Data:\")\n",
    "print(df)\n",
    "print(\"\\nCustomer Data:\")\n",
    "print(customer_df)\n",
    "print(\"\\nProduct Data:\")\n",
    "print(product_df)\n",
    "\n",
    "# Execute the parsed function definition\n",
    "globals_ = {'df': df, 'customer_df': customer_df, 'product_df': product_df}\n",
    "exec(function_definition, globals_)\n",
    "_parsed_function = globals_[function_name]\n",
    "\n",
    "# Get function signature and default parameter values\n",
    "_signature = inspect.signature(_parsed_function)\n",
    "_parameters_with_defaults = [(param.name, param.default) for param in _signature.parameters.values() if param.default != inspect.Parameter.empty]\n",
    "_default_values = dict(_parameters_with_defaults)\n",
    "\n",
    "# Prepare arguments for function execution\n",
    "_args = tuple(_default_values[arg] if arg in _default_values else globals()[arg] for arg in arguments)\n",
    "\n",
    "# Call the parsed function with appropriate arguments\n",
    "result = _parsed_function(*_args)\n",
    "\n",
    "# Print the result\n",
    "print(f\"\\nAnswer to user query:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e3b2a1-4299-454f-8094-c7cb289a59e4",
   "metadata": {},
   "source": [
    "# Results retrieval for user queries in test dataset and measuring performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc7e174e-70f2-4d3b-b6a1-1125a762d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_move_embeddings_to_gpu(query, embedder):\n",
    "    \"\"\"\n",
    "    Encodes a query and moves the embeddings to GPU.\n",
    "\n",
    "    Args:\n",
    "        query (str): The input query to encode.\n",
    "        embedder: The embedding model to use for encoding.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The encoded query embeddings on GPU.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True).to(\"cuda\")\n",
    "    logging.info(f\"\\nTime for encoding and moving embeddings to GPU: {time.time() - start_time} seconds\")\n",
    "    return query_embedding\n",
    "\n",
    "def normalize_embeddings(embeddings, util):\n",
    "    \"\"\"\n",
    "    Normalizes embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings (torch.Tensor): The embeddings to normalize.\n",
    "        util: The utility object for normalization.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The normalized embeddings.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    normalized_embeddings = util.normalize_embeddings(embeddings)\n",
    "    logging.info(f\"\\nTime for normalizing embeddings: {time.time() - start_time} seconds\")\n",
    "    return normalized_embeddings\n",
    "\n",
    "def semantic_search(query_embedding, train_corpus_embeddings, util):\n",
    "    \"\"\"\n",
    "    Performs semantic search using query embedding and train corpus embeddings.\n",
    "\n",
    "    Args:\n",
    "        query_embedding (torch.Tensor): The embedding of the query.\n",
    "        train_corpus_embeddings (torch.Tensor): The embeddings of the training corpus.\n",
    "        util: The utility object for semantic search.\n",
    "\n",
    "    Returns:\n",
    "        list: List of hits from the semantic search.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    hits = util.semantic_search(query_embedding, train_corpus_embeddings, score_function=util.dot_score, top_k=1)\n",
    "    logging.info(f\"\\nTime for semantic search: {time.time() - start_time} seconds\")\n",
    "    return hits\n",
    "\n",
    "def retrieve_context(hits, train_question_context_dict, train_questions_corpus):\n",
    "    \"\"\"\n",
    "    Retrieves context based on hits from semantic search.\n",
    "\n",
    "    Args:\n",
    "        hits (list): List of hits from semantic search.\n",
    "        train_question_context_dict (dict): Dictionary mapping questions to their contexts.\n",
    "        train_questions_corpus (list): List of questions in the training corpus.\n",
    "\n",
    "    Returns:\n",
    "        str: Retrieved context.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    retrieved_context = train_question_context_dict[train_questions_corpus[hits[0][0]['corpus_id']]]\n",
    "    logging.info(f\"\\nTime for retrieving context: {time.time() - start_time} seconds\")\n",
    "    return retrieved_context\n",
    "\n",
    "def create_prompt(test_query, retrieved_context):\n",
    "    \"\"\"\n",
    "    Creates a prompt for generating the answer.\n",
    "\n",
    "    Args:\n",
    "        test_query (str): The test query.\n",
    "        retrieved_context (str): The retrieved context.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prompt.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    ### Question: {test_query}\n",
    "\n",
    "    ### Context: {retrieved_context}\n",
    "    USE ONLY DEFINED DATAFRAMES AND INCLUDE THESE AS ARGUMENTS.\n",
    "    NO NEED TO CONSIDER ANY EXTERNAL DATA SOURCES, VARIABLES OR DATAFRAMES APART FROM THE ONES ALREADY AVAILABLE IN THE GLOBALS.\n",
    "    THE GENERATED ANSWER SHOULD BE A SINGLE PYTHON FUNCTION WITH A `def` AND `return` ONLY.\n",
    "    THE GENERATED CODE SHOULD BE COMPLETE IN ITSELF, SYNTACTICALLY CORRECT AND BE PARSED SUCCESSFULLY.\n",
    "    NO NEED TO CALL THE FUNCTION OR PRINT THE FUNCTION CALL. NO NEED TO INCLUDE MAIN CALL. JUST PROVIDING THE PYTHON FUNCTION IS SUFFICIENT.\n",
    "    \n",
    "    ### Answer:\n",
    "    \"\"\".strip()\n",
    "    return prompt\n",
    "\n",
    "def tokenize_prompt(prompt, test_tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes the prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt to tokenize.\n",
    "        test_tokenizer: The tokenizer to use.\n",
    "\n",
    "    Returns:\n",
    "        dict: Model input with tokenized prompt.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    model_input = test_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    logging.info(f\"\\nTime for tokenizing prompt: {time.time() - start_time} seconds\")\n",
    "    return model_input\n",
    "\n",
    "def generate_text(model_input, ft_model, eval_tokenizer):\n",
    "    \"\"\"\n",
    "    Generates text based on model input.\n",
    "\n",
    "    Args:\n",
    "        model_input (dict): Model input with tokenized prompt.\n",
    "        ft_model: The fine-tuned model for text generation.\n",
    "        eval_tokenizer: The tokenizer for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Generated tokens.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = ft_model.generate(\n",
    "            **model_input,\n",
    "            max_new_tokens=1024,\n",
    "            repetition_penalty=1.15,\n",
    "            pad_token_id=eval_tokenizer.eos_token_id\n",
    "        )[0]\n",
    "    logging.info(f\"\\nTime for generating text: {time.time() - start_time} seconds\")\n",
    "    return generated_tokens\n",
    "\n",
    "def decode_generated_text(generated_tokens, eval_tokenizer):\n",
    "    \"\"\"\n",
    "    Decodes generated text tokens.\n",
    "\n",
    "    Args:\n",
    "        generated_tokens (torch.Tensor): Generated tokens.\n",
    "        eval_tokenizer: The tokenizer for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        str: Decoded generated text.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    generated_text = eval_tokenizer.decode(\n",
    "        generated_tokens,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    logging.info(f\"\\nTime for decoding generated text: {time.time() - start_time} seconds\")\n",
    "    return generated_text\n",
    "\n",
    "def process_answer(generated_text):\n",
    "    \"\"\"\n",
    "    Processes the generated text to extract the answer.\n",
    "\n",
    "    Args:\n",
    "        generated_text (str): The generated text.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed answer.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    start_marker = \"### Answer:\"\n",
    "    start_index = generated_text.find(start_marker) + len(start_marker)\n",
    "\n",
    "    if start_index == -1:\n",
    "        return \"No answer found.\"\n",
    "\n",
    "    answer = generated_text[start_index:].strip()\n",
    "\n",
    "    end_marker = None\n",
    "    if \"### Explanation:\" in answer:\n",
    "        end_marker = \"### Explanation:\"\n",
    "    elif \"### \" in answer:\n",
    "        end_marker = \"### \"\n",
    "\n",
    "    if end_marker:\n",
    "        end_index = answer.find(end_marker)\n",
    "        if end_index != -1:\n",
    "            answer = answer[:end_index].strip()\n",
    "\n",
    "    end_of_function_index = answer.find(\"\\r\\n\")\n",
    "    if end_of_function_index != -1:\n",
    "        answer = answer[:end_of_function_index].strip()\n",
    "\n",
    "    end_of_function_index = answer.find(\"if __name__ ==\")\n",
    "    if end_of_function_index != -1:\n",
    "        answer = answer[:end_of_function_index].strip()\n",
    "\n",
    "    answer = answer.replace(\"\\\\\\\\\", \"\\\\\").strip().replace(\"\\\\n\", \"\\n\").rstrip('\\\\').replace(\"\\\\'\", \"'\").strip()\n",
    "    logging.info(f\"\\nTime for processing answer string: {time.time() - start_time} seconds\")\n",
    "    return answer\n",
    "\n",
    "def print_assistant_message(function_definition):\n",
    "    \"\"\"\n",
    "    Prints a message from the assistant.\n",
    "\n",
    "    Args:\n",
    "        function_definition (str): The definition of the function.\n",
    "    \"\"\"\n",
    "    print(f\"\"\"\n",
    "    Assistant:\n",
    "    {function_definition}\n",
    "    \"\"\")\n",
    "\n",
    "def execute_function(parsed_function, globals_, arguments):\n",
    "    \"\"\"\n",
    "    Executes a parsed function.\n",
    "\n",
    "    Args:\n",
    "        parsed_function (str): The name of the function to execute.\n",
    "        globals_ (dict): Global namespace dictionary.\n",
    "        arguments (tuple): Arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "        any: Result of the executed function.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    _parsed_function = globals_[parsed_function]\n",
    "    _signature = inspect.signature(_parsed_function)\n",
    "    _parameters_with_defaults = [(param.name, param.default) for param in _signature.parameters.values() if param.default != inspect.Parameter.empty]\n",
    "    _default_values = dict(_parameters_with_defaults)\n",
    "    _args = tuple(_default_values[arg] if arg in _default_values else globals()[arg] for arg in arguments)\n",
    "    result = _parsed_function(*_args)\n",
    "    logging.info(f\"\\nTime for executing function: {time.time() - start_time} seconds\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156ec9d-a8e6-4638-b003-1adf4507b072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import inspect\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Counter for tracking errors\n",
    "cnt = 0\n",
    "cnt_2 = 0\n",
    "cnt_3 = 0\n",
    "\n",
    "# Loop through test dataset\n",
    "for idx, _ in enumerate(test_dataset):\n",
    "    loop_start_time = time.time()\n",
    "    \n",
    "    # Counter for code parsing status\n",
    "    code_parse_ind = 0\n",
    "    \n",
    "    # Retrieve test sample\n",
    "    test_sample = test_dataset[idx]\n",
    "\n",
    "    # Query sentence:\n",
    "    test_query = [test_sample['messages'][1]['content']]\n",
    "    print(f\"\\nUser Query {idx+1}: {test_query[0]}\")\n",
    "    \n",
    "    # Encode the test query into embeddings and move them to GPU if available\n",
    "    query_embedding = encode_and_move_embeddings_to_gpu(test_query, embedder)\n",
    "\n",
    "    # Normalize the embeddings\n",
    "    query_embedding = normalize_embeddings(query_embedding, util)\n",
    "\n",
    "    # Find the most similar question in the training set for the test query\n",
    "    hits = semantic_search(query_embedding, train_corpus_embeddings, util)\n",
    "\n",
    "    # Retrieve the context corresponding to the most similar question\n",
    "    retrieved_context = retrieve_context(hits, train_question_context_dict, train_questions_corpus)\n",
    "\n",
    "    # Create a prompt with the test query and the retrieved context\n",
    "    prompt = create_prompt(test_query[0], retrieved_context)\n",
    "\n",
    "    # Tokenize the test prompt and prepare model input for inference\n",
    "    model_input = tokenize_prompt(prompt, test_tokenizer)\n",
    "\n",
    "    # Generate text based on the model input\n",
    "    generated_tokens = generate_text(model_input, ft_model, eval_tokenizer)\n",
    "\n",
    "    # Decode the generated tokens into text, skipping special tokens\n",
    "    generated_text = decode_generated_text(generated_tokens, eval_tokenizer)\n",
    "    \n",
    "    # Process the generated text to extract the answer\n",
    "    answer = process_answer(generated_text)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Parse the function definition and retrieve function name, arguments, and complete function definition\n",
    "        start_time = time.time()\n",
    "        function_name, arguments, body, function_definition = parse_function_definition(answer)\n",
    "        logging.info(f\"\\nTime for parsing function definition: {time.time() - start_time} seconds\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print_assistant_message(function_definition)\n",
    "        \n",
    "        code_parse_ind = 1\n",
    "        \n",
    "        globals_ = {'df': df, 'customer_df': customer_df, 'product_df': product_df}\n",
    "        \n",
    "        exec(function_definition, globals_)\n",
    "        \n",
    "        # Execute the parsed function definition\n",
    "        result = execute_function(function_name, globals_, arguments)\n",
    "        \n",
    "        # Print the result\n",
    "        print(f\"Result:\\n{result}\")\n",
    "\n",
    "    except Exception as err_1:\n",
    "        if code_parse_ind == 0:\n",
    "            print(f\"\\nGenerated Answer:\\n{answer}\")\n",
    "        cnt += 1\n",
    "        print(f\"ERROR: {err_1} | counter 1: {cnt}\")\n",
    "        \n",
    "        ##### 2nd pass #####\n",
    "        \n",
    "        error_prompt = f\"\"\"\n",
    "        ### Question: {test_query}. Do not use {err_1} in your response.\n",
    "        \n",
    "        ### Context: {retrieved_context}\n",
    "        \n",
    "        ### Answer:\n",
    "        \"\"\".strip()\n",
    "        model_input = tokenize_prompt(error_prompt, test_tokenizer)\n",
    "        generated_tokens = generate_text(model_input, ft_model, eval_tokenizer)\n",
    "        generated_text = decode_generated_text(generated_tokens, eval_tokenizer)\n",
    "        answer = process_answer(generated_text)\n",
    "        try:\n",
    "            function_name, arguments, body, function_definition = parse_function_definition(answer)\n",
    "            print_assistant_message(function_definition)\n",
    "            code_parse_ind = 1\n",
    "            globals_ = {'df': df, 'customer_df': customer_df, 'product_df': product_df}\n",
    "            exec(function_definition, globals_)\n",
    "            result = execute_function(function_name, globals_, arguments)\n",
    "            print(f\"Result:\\n{result}\")\n",
    "        except Exception as err_2:\n",
    "            if code_parse_ind == 0:\n",
    "                print(f\"\\nGenerated Answer:\\n{answer}\")\n",
    "            cnt_2 += 1\n",
    "            print(f\"ERROR: {err_2} | counter 2: {cnt_2}\")\n",
    "            \n",
    "            ##### 3rd pass #####\n",
    "        \n",
    "            error_prompt = f\"\"\"\n",
    "            ### Question: {test_query}. Do not use {err_1} and {err_2} in your response.\n",
    "\n",
    "            ### Context: {retrieved_context}\n",
    "\n",
    "            ### Answer:\n",
    "            \"\"\".strip()\n",
    "            model_input = tokenize_prompt(error_prompt, test_tokenizer)\n",
    "            generated_tokens = generate_text(model_input, ft_model, eval_tokenizer)\n",
    "            generated_text = decode_generated_text(generated_tokens, eval_tokenizer)\n",
    "            answer = process_answer(generated_text)\n",
    "            try:\n",
    "                function_name, arguments, body, function_definition = parse_function_definition(answer)\n",
    "                print_assistant_message(function_definition)\n",
    "                code_parse_ind = 1\n",
    "                globals_ = {'df': df, 'customer_df': customer_df, 'product_df': product_df}\n",
    "                exec(function_definition, globals_)\n",
    "                result = execute_function(function_name, globals_, arguments)\n",
    "                print(f\"Result:\\n{result}\")\n",
    "            except Exception as err_3:\n",
    "                if code_parse_ind == 0:\n",
    "                    print(f\"\\nGenerated Answer:\\n{answer}\")\n",
    "                cnt_3 += 1\n",
    "                print(f\"ERROR: {err_3} | counter 3: {cnt_3}\")\n",
    "            \n",
    "        #####\n",
    "\n",
    "    logging.info(f\"\\nTotal loop execution time: {time.time() - loop_start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ff746b5-c4f2-4128-8c00-4f4ceba433db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total errors (Pass 1): 10\n",
      "\n",
      "Total errors (Pass 2): 7\n",
      "\n",
      "Total errors (Pass 3): 5\n",
      "\n",
      "Total test samples: 42\n",
      "\n",
      "1st pass Accuracy: 76.19047619047619%\n",
      "\n",
      "2nd pass Accuracy: 83.33333333333334%\n",
      "\n",
      "3rd pass Accuracy: 88.09523809523809%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTotal errors (Pass 1): {cnt}\")\n",
    "print(f\"\\nTotal errors (Pass 2): {cnt_2}\")\n",
    "print(f\"\\nTotal errors (Pass 3): {cnt_3}\")\n",
    "print(f\"\\nTotal test samples: {len(test_dataset)}\")\n",
    "print(f\"\\n1st pass Accuracy: {(1-(cnt/len(test_dataset)))*100}%\")\n",
    "print(f\"\\n2nd pass Accuracy: {(1-(cnt_2/len(test_dataset)))*100}%\")\n",
    "print(f\"\\n3rd pass Accuracy: {(1-(cnt_3/len(test_dataset)))*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8949610e-98bc-4c38-9c5d-3618a81acedc",
   "metadata": {},
   "source": [
    "We observe an improvement in accuracy with each subsequent pass. This improvement suggests that the error handling strategy implemented, which includes generating a new response excluding previously encountered errors, is effective in enhancing the overall accuracy of the system. However, despite the improvement, there are still errors encountered during the testing phase. This indicates that there might be inherent limitations in the model. Further analysis of the types of errors encountered and their root causes could provide insights into areas for improvement. If we train the model with these erroneous observations as additional context, the model can provide better accuracy in zero-shot. Additionally, it's important to consider the complexity and diversity of the test dataset. This leads to the insight that the dataset contains a wide range of scenarios and edge cases, achieving a higher accuracy might require more sophisticated error handling mechanisms or model enhancements. While the iterative error handling strategy has led to an improvement in accuracy, continued refinement and evaluation are necessary to further enhance the performance of the system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python new (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
