{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33dc1edc-3384-45c0-b683-f15afd331484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.33.2\n",
      "  Downloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken==0.4.0\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.2) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers==4.33.2)\n",
      "  Using cached huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.2) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.2) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.33.2)\n",
      "  Using cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.2) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.2)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers==4.33.2)\n",
      "  Using cached safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.2) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.2) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.2) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.2) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.2) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.2) (2023.7.22)\n",
      "Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "Using cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Using cached safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, safetensors, regex, tiktoken, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.20.3 regex-2023.12.25 safetensors-0.4.2 tiktoken-0.4.0 tokenizers-0.13.3 transformers-4.33.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.33.2 tiktoken==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125a0968-6a9a-4b2a-bdd0-38eecb97b7e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "2024-02-23 13:25:41.720526: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-23 13:25:41.762446: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-23 13:25:41.762499: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-23 13:25:41.763762: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-23 13:25:41.770813: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-23 13:25:42.679546: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# this function prints hello world\n",
      "\n",
      "def hello_world():\n",
      "    print(\"Hello World\")\n",
      "\n",
      "hello_world()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-2B-mono\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-2B-mono\").to(device)\n",
    "inputs = tokenizer(\"# this function prints hello world\", return_tensors=\"pt\").to(device)\n",
    "sample = model.generate(**inputs, max_length=128)\n",
    "print(tokenizer.decode(sample[0], truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96d3e397-55d6-400b-b157-60fefa68a4a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# this function prints the customer base\n",
      "# it takes the customer base as a parameter\n",
      "# and prints the customer base\n",
      "def print_customer_base(customer_base):\n",
      "    print(\"\\nCustomer base:\")\n",
      "    for customer in customer_base:\n",
      "        print(customer)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"# this function prints the customer base\", return_tensors=\"pt\").to(device)\n",
    "sample = model.generate(**inputs, max_length=128)\n",
    "print(tokenizer.decode(sample[0], truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9629ec-4c72-4702-ac32-e62da632af4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# this function calculates the customer base of the customer\n",
      "# the customer base is the number of customers who have\n",
      "# a rating of 5 or more\n",
      "\n",
      "def customer_base(df):\n",
      "    # create a new column called customer base\n",
      "    df['customer base'] = 0\n",
      "    # loop through the dataframe\n",
      "    for i in range(len(df)):\n",
      "        # if the rating is 5 or more\n",
      "        if df['rating'][i] >= 5:\n",
      "            # add 1 to the customer base\n",
      "            df['customer base'][i] = 1\n",
      "    # return\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"# this function calculates the customer base\", return_tensors=\"pt\").to(device)\n",
    "sample = model.generate(**inputs, max_length=128)\n",
    "print(tokenizer.decode(sample[0], truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c788a6-c2a3-4235-b130-9aa9e868b875",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are a proficient python developer. Write Pandas code to get the answer to the user's question. Respond with syntactically correct & concise code for to the question below.  Make sure you follow these rules:\n",
      "    1. Use only pandas operations for the solution.\n",
      "    2. Store the answer in a variable called'result'.\n",
      "    3. Place your code between the [PYTHON] and [/PYTHON] tags.\n",
      "    4. Ensure all the requirements in the question are met.\n",
      "    5. In case you are not sure, say 'I DO NOT KNOW!'\n",
      "    Question:\n",
      "    # this function prints hello world\n",
      "\n",
      "    Response:\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 146, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "user_query = \"# this function prints hello world\"\n",
    "prompt_template=f\"\"\"\n",
    "    You are a proficient python developer. Write Pandas code to get the answer to the user's question. Respond with syntactically correct & concise code for to the question below.  Make sure you follow these rules:\n",
    "    1. Use only pandas operations for the solution.\n",
    "    2. Store the answer in a variable called 'result'.\n",
    "    3. Place your code between the [PYTHON] and [/PYTHON] tags.\n",
    "    4. Ensure all the requirements in the question are met.\n",
    "    5. In case you are not sure, say 'I DO NOT KNOW!'\n",
    "    Question:\n",
    "    {user_query}\n",
    "\n",
    "    Response:\n",
    "\"\"\"\n",
    "inputs = tokenizer(prompt_template, return_tensors=\"pt\").to(device)\n",
    "sample = model.generate(**inputs, max_length=128)\n",
    "print(tokenizer.decode(sample[0], truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
