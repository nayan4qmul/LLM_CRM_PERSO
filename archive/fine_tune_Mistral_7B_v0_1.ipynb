{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86bcf62-65e9-4e37-884d-9ee16936153d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfbae3d7863443686b470a0d4d7ffdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e461b625786447b4b44b2a499118ff2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "I'd like to know the top emerging trends in product categories based on recent transactions within the last 30 days. Can you provide me with the top 3 product categories with the highest purchase counts?\n",
      "\n",
      "Context:\n",
      "\n",
      "You are a Python function generator. Users will ask you questions in English, \n",
      "and you will produce a Python function as answer to the question based on the provided CONTEXT.\n",
      "\n",
      "CONTEXT:\n",
      "Pandas DataFrame df containing transaction data with columns order_id, user_id, item_id, timestamp, score.\n",
      "order_id takes string datatype and identifies the order.\n",
      "user_id takes string datatype and identifies the customer.\n",
      "item_id takes string datatype that identifies the product.\n",
      "timestamp takes timestamp datatype and represents the datetimestamp of transaction.\n",
      "score takes float datatype and represents the score of the transaction.\n",
      "Note that a customer can make multiple transactions for a product but the customer product pair will be just one entry for each order.\n",
      "Pandas DataFrame product_dfcontaining product data with columns item_id, product_category.\n",
      "item_id takes string datatype that identifies the product.\n",
      "product_category takes string datatype and represents the product category in the product hierarchy.\n",
      "Note that df can be joined with product_df on item_id.\n",
      "\n",
      "\n",
      "Original Answer:\n",
      "def emerging_trends(df, product_df, time_window='30D', top_n=3):\n",
      "    df = df.merge(product_df, on='item_id')\n",
      "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
      "    start_date = df['timestamp'].max() - pd.to_timedelta(time_window)\n",
      "    recent_transactions = df[df['timestamp'] >= start_date]\n",
      "    trend_categories = recent_transactions['product_category'].value_counts().head(top_n).reset_index()\n",
      "    trend_categories.columns = ['product_category', 'purchase_count']\n",
      "    return trend_categories\n",
      "\n",
      "\n",
      "Generated Answer:\n",
      "[INST]\n",
      "\n",
      "You are a Python function generator. Users will ask you questions in English, and you will produce a Python function as an answer to the question.\n",
      "\n",
      "CONTEXT:\n",
      "Pandas DataFrame df containing transaction data with columns order_id, user_id, item_id, timestamp, score.\n",
      "order_id takes string datatype and identifies the order.\n",
      "user_id takes string datatype and identifies the customer.\n",
      "item_id takes string datatype that identifies the product.\n",
      "timestamp takes timestamp datatype and represents the datetimestamp of transaction.\n",
      "score takes float datatype and represents the score of the transaction.\n",
      "Note that a customer can make multiple transactions for a product but the customer product pair will be just one entry for each order.\n",
      "Pandas DataFrame product_df containing product data with columns item_id, product_category.\n",
      "item_id takes string datatype that identifies the product.\n",
      "product_category takes string datatype and represents the product category in the product hierarchy.\n",
      "Note that df can be joined with product_df on item_id.\n",
      "\n",
      "<</INST>\n",
      "\n",
      "Iâ€™d like to know the top emerging trends in product categories based on recent transactions within the last 30 days. Can you provide me with the top 3 product categories with the highest purchase counts? [/INST]\n",
      "\n",
      "[OUT]\n",
      "\n",
      "def top_emerging_trends(df, product_df, product_category_column, time_period):\n",
      "    # Join the transaction data with the product data\n",
      "    df_joined = pd.merge(df, product_df, on='item_id')\n",
      "\n",
      "    # Filter out transactions within the given time period\n",
      "    df_filtered = df_joined[df_joined['timestamp'] >= time_period].groupby('item_id')['timestamp'].count()\n",
      "\n",
      "    # Create a dictionary of product categories with purchase counts\n",
      "    product_categories = df_filtered.groupby(product_df[product_category_column].rename('product_category')).agg({'timestamp': 'count'}).to_dict()\n",
      "\n",
      "    # Sort the dictionary by purchase counts in descending order\n",
      "    sorted_categories = {k: v for k, v in sorted(product_categories.items(), key=lambda x: x[1], reverse=True)}\n",
      "\n",
      "    # Return the top 3 product categories with the highest purchase counts\n",
      "    return sorted_categories['product_category'][:3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Additional references: \n",
    "# Mistral 7B: https://arxiv.org/pdf/2310.06825.pdf (Mistral.AI)\n",
    "# https://www.datacamp.com/tutorial/mistral-7b-tutorial\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    pipeline, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "\n",
    "# Define the model ID and load the tokenizer\n",
    "\n",
    "#https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer\n",
    "model_id = 'mistralai/Mistral-7B-v0.1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "######################################################################\n",
    "# Configure BitsAndBytes (BnB) quantization                          #\n",
    "# for advanced usage to allow model running in 4-bit precision       #\n",
    "#                                                                    #\n",
    "# Ref: https://huggingface.co/blog/4bit-transformers-bitsandbytes    #\n",
    "#                                                                    #\n",
    "# Dettmers, T., Pagnoni, A., Holtzman, A. and Zettlemoyer, L. (2023).#\n",
    "# QLoRA: Efficient Finetuning of Quantized LLMs.                     #\n",
    "# [online] arXiv.org. Available at: https://arxiv.org/abs/2305.14314.#\n",
    "######################################################################\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # loads model in 4 bit\n",
    "    bnb_4bit_use_double_quant=True,       # uses second quantization to save \n",
    "                                          # an additional 0.4 bits per parameter\n",
    "    bnb_4bit_quant_type='nf4',            # normalised float 4 bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # faster training\n",
    ")\n",
    "\n",
    "# Load the model with specified quantization config\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='auto',             # accelerate handles the device map computation\n",
    "    torch_dtype=torch.bfloat16,    # better manage memory usage at the cost of performance\n",
    "    quantization_config=bnb_config # bits and bytes quantization configuration\n",
    ")\n",
    "\n",
    "# Add padding tokens to the right side of the input sequence\n",
    "# if it is shorter than the maximum length to avoid warnings\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# Create text generation pipeline (an automated pipeline for chat)\n",
    "# https://huggingface.co/docs/transformers/en/chat_templating#is-there-an-automated-pipeline-for-chat\n",
    "# https://huggingface.co/docs/transformers/v4.39.2/en/main_classes/pipelines#transformers.TextGenerationPipeline\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load dataset and select a random index from the dataset as sample to test text generation result\n",
    "\n",
    "# https://huggingface.co/docs/datasets/en/loading#json\n",
    "dataset = load_dataset(\n",
    "    'json',\n",
    "    data_files='../data/test_CRM_data.json',\n",
    "    split='train'\n",
    ")\n",
    "idx = randint(0, len(dataset))\n",
    "\n",
    "# Prepare prompt for generation and generate text based on prompt\n",
    "\n",
    "# https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    dataset[idx][\"messages\"][:2],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "##### WARNING #####\n",
    "# Ignore the below warning for now as the default chat template is applicable for the given model.\n",
    "# No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. \n",
    "# If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. \n",
    "# See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
    "##### ####### #####\n",
    "\n",
    "# Enabling the below print statements help understand the different ways the chat template can be implemented\n",
    "\n",
    "# print(\n",
    "#     tokenizer.decode(\n",
    "#         tokenizer.apply_chat_template(\n",
    "#             dataset[idx][\"messages\"][:2], \n",
    "#             tokenize=True, \n",
    "#             add_generation_prompt=True, \n",
    "#             return_tensors=\"pt\"\n",
    "#         )[0]\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# print(\n",
    "#     model.generate(\n",
    "#         tokenizer.apply_chat_template(\n",
    "#             dataset[idx][\"messages\"][:2], \n",
    "#             tokenize=True, \n",
    "#             add_generation_prompt=True, \n",
    "#             return_tensors=\"pt\"\n",
    "#         ), \n",
    "#         max_new_tokens=128,\n",
    "#         pad_token_id = tokenizer.eos_token_id\n",
    "#     )[0]\n",
    "# )\n",
    "\n",
    "# print(\n",
    "#     tokenizer.decode(\n",
    "#         model.generate(\n",
    "#             tokenizer.apply_chat_template(\n",
    "#                 dataset[idx][\"messages\"][:2], \n",
    "#                 tokenize=True, \n",
    "#                 add_generation_prompt=True, \n",
    "#                 return_tensors=\"pt\"\n",
    "#             ), \n",
    "#             max_new_tokens=128,\n",
    "#             pad_token_id = tokenizer.eos_token_id\n",
    "#         )[0]\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# print(\n",
    "#     pipe(\n",
    "#         prompt, \n",
    "#         max_new_tokens=128, \n",
    "#         pad_token_id=tokenizer.eos_token_id\n",
    "#     )[0]['generated_text']\n",
    "# )\n",
    "\n",
    "# https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=1024,                      # maximum numbers of tokens to generate upto 4096\n",
    "    do_sample=True,                           # do not use sampling\n",
    "    temperature=0.95,                         # control randomness\n",
    "    top_k=10,                                 # sample from most likely next tokens at each step\n",
    "    top_p=0.95,                               # cumulative prob. cutoff for token selection\n",
    "    eos_token_id=pipe.tokenizer.eos_token_id, # end of sequence token\n",
    "    pad_token_id=pipe.tokenizer.eos_token_id  # padding token - for enabling open-ended generation\n",
    ")\n",
    "\n",
    "# Print query, context, original answer and generated answer\n",
    "print(f\"Query:\\n{dataset[idx]['messages'][1]['content']}\\n\")\n",
    "print(f\"Context:\\n{dataset[idx]['messages'][0]['content']}\\n\")\n",
    "original_answer = dataset[idx]['messages'][2]['content'].replace(\"\\\\\\\\\", \"\\\\\").strip().replace(\"\\\\n\", \"\\n\").rstrip('\\\\').replace(\"\\\\'\", \"'\")\n",
    "print(f\"Original Answer:\\n{original_answer}\\n\")\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e8b3f6-7ff3-4aae-8b6e-48bead424ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "\n",
    "# Get the print statements of query, context, original answer and generated answer from stdout\n",
    "print(f\"Query:\\n{dataset[idx]['messages'][1]['content']}\\n\")\n",
    "print(f\"Context:\\n{dataset[idx]['messages'][0]['content']}\\n\")\n",
    "original_answer = dataset[idx]['messages'][2]['content'].replace(\"\\\\\\\\\", \"\\\\\").strip().replace(\"\\\\n\", \"\\n\").rstrip('\\\\').replace(\"\\\\'\", \"'\")\n",
    "print(f\"Original Answer:\\n{original_answer}\\n\")\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b940709-3ce6-48a8-a243-d50bca383ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the captured output to a text file\n",
    "with open('../data/random_untuned_model_gen_data.txt', 'w') as file:\n",
    "    file.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6fc2429-4a65-46e8-972e-58cba015dfbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0 is in progress.\n",
      "Index: 1 is in progress.\n",
      "Index: 2 is in progress.\n",
      "Index: 3 is in progress.\n",
      "Index: 4 is in progress.\n",
      "Index: 5 is in progress.\n",
      "Index: 6 is in progress.\n",
      "Index: 7 is in progress.\n",
      "Index: 8 is in progress.\n",
      "Index: 9 is in progress.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/LLM_CRM_PERSO/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 10 is in progress.\n",
      "Index: 11 is in progress.\n",
      "Index: 12 is in progress.\n",
      "Index: 13 is in progress.\n",
      "Index: 14 is in progress.\n",
      "Index: 15 is in progress.\n",
      "Index: 16 is in progress.\n",
      "Index: 17 is in progress.\n",
      "Index: 18 is in progress.\n",
      "Index: 19 is in progress.\n",
      "Index: 20 is in progress.\n",
      "Index: 21 is in progress.\n",
      "Index: 22 is in progress.\n",
      "Index: 23 is in progress.\n",
      "Index: 24 is in progress.\n",
      "Index: 25 is in progress.\n",
      "Index: 26 is in progress.\n",
      "Index: 27 is in progress.\n",
      "Index: 28 is in progress.\n",
      "Index: 29 is in progress.\n",
      "Index: 30 is in progress.\n",
      "Index: 31 is in progress.\n",
      "Index: 32 is in progress.\n",
      "Index: 33 is in progress.\n",
      "Index: 34 is in progress.\n",
      "Index: 35 is in progress.\n",
      "Index: 36 is in progress.\n",
      "Index: 37 is in progress.\n",
      "Index: 38 is in progress.\n",
      "Index: 39 is in progress.\n",
      "Index: 40 is in progress.\n",
      "Index: 41 is in progress.\n"
     ]
    }
   ],
   "source": [
    "# Capturing the generated output from untuned model for all samples from dataset.\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "\n",
    "def generate_answer(dataset, idx):\n",
    "    \"\"\"\n",
    "    Generate an answer using the given dataset and index.\n",
    "\n",
    "    Args:\n",
    "    - dataset (Dataset): pyarrow Dataset containing data for generating answers.\n",
    "    - idx (int): Index of the dataset to use.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the query, context, original answer, and generated answer.\n",
    "    \"\"\"\n",
    "    # Create prompt using the query and context in the dataset\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        dataset[idx][\"messages\"][:2],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Generate answer using the prompt\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=1024,                      # maximum numbers of tokens to generate upto 4096\n",
    "        do_sample=True,                           # do not use sampling\n",
    "        temperature=0.95,                         # control randomness\n",
    "        top_k=10,                                 # sample from most likely next tokens at each step\n",
    "        top_p=0.95,                               # cumulative prob. cutoff for token selection\n",
    "        eos_token_id=pipe.tokenizer.eos_token_id, # end of sequence token\n",
    "        pad_token_id=pipe.tokenizer.eos_token_id  # padding token - for enabling open-ended generation\n",
    "    )\n",
    "    \n",
    "    # Extract relevant information from the dataset\n",
    "    query = dataset[idx]['messages'][1]['content']\n",
    "    context = dataset[idx]['messages'][0]['content']\n",
    "    original_answer = dataset[idx]['messages'][2]['content'].replace(\"\\\\\\\\\", \"\\\\\").strip().replace(\"\\\\n\", \"\\n\").rstrip('\\\\').replace(\"\\\\'\", \"'\")\n",
    "    \n",
    "    # Extract generated answer from the outputs\n",
    "    generated_answer = outputs[0]['generated_text'][len(prompt):].strip()\n",
    "    \n",
    "    return query, context, original_answer, generated_answer\n",
    "    \n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\n",
    "    'json',\n",
    "    data_files='../data/test_CRM_data.json',\n",
    "    split='train'\n",
    ")\n",
    "\n",
    "##### WARNING #####\n",
    "# Ignore this warning for now as it may not be the best way of implementation.\n",
    "# The below warning will starting coming after 10 calls.\n",
    "# UserWarning: You seem to be using the pipelines sequentially on GPU. \n",
    "# In order to maximize efficiency please use a dataset\n",
    "#  warnings.warn(\n",
    "##### ####### #####\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    print(f\"Index: {idx} is in progress.\")\n",
    "    query, context, original_answer, generated_answer = generate_answer(dataset, idx)\n",
    "    data.append([query, context, original_answer, generated_answer])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Query', 'Context', 'Original_Answer', 'Generated_Answer'])\n",
    "df.to_csv('../data/untuned_model_gen_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d587252e-711c-4064-b2a8-cbcdfcedfa14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/jovyan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "  token=\"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\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbefa58-6fbc-4753-965a-37dfb3c6a5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from trl import (\n",
    "    setup_chat_format, \n",
    "    SFTTrainer\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    concatenate_datasets\n",
    ")\n",
    "from random import randint\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    load_in_4bit=True, ### Need to test this\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    r=256,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../models/code-mistral-7b-text-to-python\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=3,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"../data/train_CRM_data.json\", split='train')\n",
    "val_dataset = load_dataset(\"json\", data_files=\"../data/val_CRM_data.json\", split='train')\n",
    "\n",
    "max_seq_length = 1024\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb8461b-8bba-4d42-a1e7-ffcbdc85ca7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eb72ae-a8d5-44fc-8468-d032ec972011",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5123c-3390-451d-84d5-e9ff0ba08f5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model_id = \"../models/code-mistral-7b-text-to-python\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"../data/test_CRM_data.json\", split=\"train\")\n",
    "idx = randint(0, len(dataset))\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(dataset[idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n",
    "\n",
    "print(f\"Query:\\n{dataset[idx]['messages'][1]['content']}\")\n",
    "print(f\"Original Answer:\\n{dataset[idx]['messages'][2]['content']}\")\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python new (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
